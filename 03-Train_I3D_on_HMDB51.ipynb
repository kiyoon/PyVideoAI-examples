{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44eb80d-bdd5-4f1f-954f-c8c10301d679",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now the fun part begins. **Let's train the I3D network on the HMDB-51 dataset!**  \n",
    "\n",
    "First, we want to download the pre-trained weights from the [MODEL_ZOO.md](https://github.com/kiyoon/PyVideoAI/blob/master/MODEL_ZOO.md)  \n",
    "We'll use the I3D pretrained on the Kinetics-400 dataset, with 8-frame input.  \n",
    "\n",
    "Note that the path to the pretrained weights is defined in `model_configs/i3d_resnet50.py` as below.  \n",
    "```python\n",
    "pretrained_path_8x8 = os.path.join(DATA_DIR, 'pretrained', 'I3D_8x8_R50.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b00c0c1-2cb7-41fa-aff9-916f779a8cda",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYVIDEOAI_DIR=/home/kiyoon/project/PyVideoAI\n",
      "env: DATA_DIR=/home/kiyoon/project/PyVideoAI/data\n",
      "env: HDD_PATH=/storage/kiyoon\n"
     ]
    }
   ],
   "source": [
    "# Environments for future use\n",
    "\n",
    "from pyvideoai.config import PYVIDEOAI_DIR, DATA_DIR\n",
    "%env PYVIDEOAI_DIR=$PYVIDEOAI_DIR\n",
    "%env DATA_DIR=$DATA_DIR\n",
    "\n",
    "# !! CHANGE BELOW\n",
    "%env HDD_PATH=/storage/kiyoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6efdac-059c-46d4-a419-8856d6b8c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-14 03:17:54--  https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/I3D_8x8_R50.pkl\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 224598662 (214M) [application/octet-stream]\n",
      "Saving to: ‘/home/kiyoon/project/PyVideoAI/data/pretrained/I3D_8x8_R50.pkl’\n",
      "\n",
      "I3D_8x8_R50.pkl     100%[===================>] 214.19M  11.4MB/s    in 20s     \n",
      "\n",
      "2021-06-14 03:18:15 (10.6 MB/s) - ‘/home/kiyoon/project/PyVideoAI/data/pretrained/I3D_8x8_R50.pkl’ saved [224598662/224598662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Link the pretrained weight directory to HDD\n",
    "!mkdir -p \"$HDD_PATH/pretrained\"\n",
    "!ln -s \"$HDD_PATH/pretrained\" \"$DATA_DIR/\"\n",
    "\n",
    "# Download\n",
    "!wget https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/I3D_8x8_R50.pkl -P \"$DATA_DIR/pretrained\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c762f-6e2b-4841-91c3-82a299e00600",
   "metadata": {},
   "source": [
    "Now we have our datasets processed, and the pretrained weights ready.  \n",
    "We want to use three config files.  \n",
    "- `hmdb.py` in `dataset_configs`,  \n",
    "- `i3d_resnet50.py` in `model_configs`,  \n",
    "- `hmdb/i3d_resnet50-crop224_lr001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1.py` in `exp_configs`.\n",
    "\n",
    "You'll need at least 6GiB of VRAM on your GPU.  \n",
    "If this isn't the case or you have more, change the batch size in `exp_configs/hmdb/i3d_resnet50-crop224_lr001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790f1cd2-a928-45fa-8bc6-3b9058d2f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Setup Telegram bot to report the experiment stats\n",
    "import os\n",
    "if not os.path.isfile(f\"{PYVIDEOAI_DIR}/tools/key.ini\"):\n",
    "    !cp \"$PYVIDEOAI_DIR/tools/key.ini\"{.template,}\n",
    "# EDIT the `tools/key.ini` file on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bf4ff2-68f2-4f49-a76a-eef25f45ef8a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment_utils.experiment_builder:  103 - INFO - Telegram bot initialised with keys in /home/kiyoon/project/PyVideoAI/tools/key.ini and using the bot number 0\n",
      "pyvideoai.train_multiprocess:  121 - INFO - git hash: 48a5d20dc2e17fcd7b4343cc562f043b9839d6e2\n",
      "pyvideoai.train_multiprocess:  125 - INFO - args: {\n",
      "    \"local_world_size\": 1,\n",
      "    \"shard_id\": 0,\n",
      "    \"num_shards\": 1,\n",
      "    \"init_method\": \"tcp://localhost:19999\",\n",
      "    \"backend\": \"nccl\",\n",
      "    \"num_epochs\": 100,\n",
      "    \"experiment_root\": \"/home/kiyoon/project/PyVideoAI/data/experiments\",\n",
      "    \"dataset\": \"hmdb\",\n",
      "    \"model\": \"i3d_resnet50\",\n",
      "    \"experiment_name\": \"crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1\",\n",
      "    \"dataset_channel\": null,\n",
      "    \"model_channel\": null,\n",
      "    \"experiment_channel\": null,\n",
      "    \"save_mode\": \"last_and_peaks\",\n",
      "    \"load_epoch\": null,\n",
      "    \"seed\": 12,\n",
      "    \"multi_crop_val_freq\": 5,\n",
      "    \"telegram_post_freq\": 5,\n",
      "    \"telegram_bot_idx\": 0,\n",
      "    \"dataloader_num_workers\": 4\n",
      "}\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset train...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 3570) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/train1.csv\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset test...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 1530) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/test1.csv\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset test...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 7650) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/test1.csv\n",
      "pyvideoai.utils.misc:   52 - INFO - Model:\n",
      "ResNetModel(\n",
      "  (s1): VideoModelStem(\n",
      "    (pathway0_stem): ResNetBasicStem(\n",
      "      (conv): Conv3d(3, 64, kernel_size=[5, 7, 7], stride=[1, 2, 2], padding=[2, 3, 3], bias=False)\n",
      "      (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (pool_layer): MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (s2): ResStage(\n",
      "    (pathway0_res0): ResBlock(\n",
      "      (branch1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=[1, 1, 1], bias=False)\n",
      "      (branch1_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(64, 64, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(64, 64, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(64, 256, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res1): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(256, 64, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(64, 64, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(64, 256, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res2): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(256, 64, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(64, 64, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(64, 256, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pathway0_pool): MaxPool3d(kernel_size=[2, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=1, ceil_mode=False)\n",
      "  (s3): ResStage(\n",
      "    (pathway0_res0): ResBlock(\n",
      "      (branch1): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=[1, 2, 2], bias=False)\n",
      "      (branch1_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(256, 128, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(128, 128, kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(128, 512, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res1): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(512, 128, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(128, 128, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(128, 512, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res2): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(512, 128, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(128, 128, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(128, 512, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res3): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(512, 128, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(128, 128, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(128, 512, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (s4): ResStage(\n",
      "    (pathway0_res0): ResBlock(\n",
      "      (branch1): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=[1, 2, 2], bias=False)\n",
      "      (branch1_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(512, 256, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res1): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res2): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res3): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res4): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res5): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (s5): ResStage(\n",
      "    (pathway0_res0): ResBlock(\n",
      "      (branch1): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=[1, 2, 2], bias=False)\n",
      "      (branch1_bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 512, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(512, 512, kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(512, 2048, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res1): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(2048, 512, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(512, 512, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(512, 2048, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res2): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(2048, 512, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(512, 512, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(512, 2048, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (head): ResNetBasicHead(\n",
      "    (pathway0_avgpool): AvgPool3d(kernel_size=[4, 7, 7], stride=1, padding=0)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (projection): Linear(in_features=2048, out_features=51, bias=True)\n",
      "    (act): Softmax(dim=4)\n",
      "  )\n",
      ")\n",
      "pyvideoai.utils.misc:   53 - INFO - Params: 27,328,371\n",
      "pyvideoai.utils.misc:   54 - INFO - Mem: 104.4853515625 MiB\n",
      "pyvideoai.utils.misc:   55 - INFO - nvidia-smi\n",
      "pyvideoai.utils.misc:   56 - INFO - Tue Jun 15 07:03:50 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3090    Off  | 00000000:23:00.0 Off |                  N/A |\n",
      "| 55%   52C    P2   120W / 370W |   1213MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 3090    Off  | 00000000:2D:00.0 Off |                  N/A |\n",
      "| 53%   60C    P0    45W / 370W |      0MiB / 24251MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1353154      C   ...3/envs/videoai/bin/python     1211MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res_conv1_bn_b: (64,) => s1.pathway0_stem.bn.bias: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res_conv1_bn_s: (64,) => s1.pathway0_stem.bn.weight: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2a_bn_rm: (512,) => s5.pathway0_res2.branch2.a_bn.running_mean: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res4.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2c_bn_rm: (2048,) => s5.pathway0_res1.branch2.c_bn.running_mean: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2c_bn_s: (512,) => s3.pathway0_res1.branch2.c_bn.weight: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2b_w: (512, 512, 1, 3, 3) => s5.pathway0_res1.branch2.b.weight: (512, 512, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2a_bn_riv: (128,) => s3.pathway0_res0.branch2.a_bn.running_var: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2b_bn_rm: (64,) => s2.pathway0_res2.branch2.b_bn.running_mean: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2c_bn_riv: (2048,) => s5.pathway0_res0.branch2.c_bn.running_var: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2a_w: (128, 512, 3, 1, 1) => s3.pathway0_res2.branch2.a.weight: (128, 512, 3, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2c_bn_rm: (1024,) => s4.pathway0_res5.branch2.c_bn.running_mean: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2c_w: (512, 128, 1, 1, 1) => s3.pathway0_res2.branch2.c.weight: (512, 128, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2c_bn_rm: (1024,) => s4.pathway0_res0.branch2.c_bn.running_mean: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2a_bn_riv: (64,) => s2.pathway0_res2.branch2.a_bn.running_var: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2a_w: (256, 1024, 1, 1, 1) => s4.pathway0_res5.branch2.a.weight: (256, 1024, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2a_bn_b: (64,) => s2.pathway0_res2.branch2.a_bn.bias: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2a_bn_s: (64,) => s2.pathway0_res2.branch2.a_bn.weight: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2a_bn_rm: (128,) => s3.pathway0_res2.branch2.a_bn.running_mean: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res0.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2a_bn_b: (128,) => s3.pathway0_res2.branch2.a_bn.bias: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2b_bn_riv: (256,) => s4.pathway0_res1.branch2.b_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2b_bn_b: (256,) => s4.pathway0_res4.branch2.b_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2c_bn_b: (512,) => s3.pathway0_res1.branch2.c_bn.bias: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2c_bn_riv: (1024,) => s4.pathway0_res0.branch2.c_bn.running_var: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2b_bn_riv: (128,) => s3.pathway0_res3.branch2.b_bn.running_var: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2a_bn_riv: (512,) => s5.pathway0_res1.branch2.a_bn.running_var: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2b_bn_s: (256,) => s4.pathway0_res4.branch2.b_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch1_bn_rm: (2048,) => s5.pathway0_res0.branch1_bn.running_mean: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2a_bn_rm: (64,) => s2.pathway0_res0.branch2.a_bn.running_mean: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2a_bn_rm: (256,) => s4.pathway0_res4.branch2.a_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2a_bn_rm: (256,) => s4.pathway0_res0.branch2.a_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch1_w: (512, 256, 1, 1, 1) => s3.pathway0_res0.branch1.weight: (512, 256, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2c_bn_riv: (1024,) => s4.pathway0_res5.branch2.c_bn.running_var: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2a_bn_rm: (64,) => s2.pathway0_res1.branch2.a_bn.running_mean: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2a_bn_rm: (128,) => s3.pathway0_res3.branch2.a_bn.running_mean: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2a_w: (256, 1024, 1, 1, 1) => s4.pathway0_res1.branch2.a.weight: (256, 1024, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch1_bn_rm: (512,) => s3.pathway0_res0.branch1_bn.running_mean: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res1.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res5.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2c_bn_b: (512,) => s3.pathway0_res3.branch2.c_bn.bias: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2c_bn_s: (512,) => s3.pathway0_res3.branch2.c_bn.weight: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2a_w: (512, 2048, 3, 1, 1) => s5.pathway0_res1.branch2.a.weight: (512, 2048, 3, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2a_bn_riv: (512,) => s5.pathway0_res0.branch2.a_bn.running_var: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2c_bn_riv: (256,) => s2.pathway0_res1.branch2.c_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2b_bn_b: (256,) => s4.pathway0_res5.branch2.b_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2b_bn_s: (256,) => s4.pathway0_res5.branch2.b_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2c_bn_b: (1024,) => s4.pathway0_res4.branch2.c_bn.bias: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2c_w: (2048, 512, 1, 1, 1) => s5.pathway0_res1.branch2.c.weight: (2048, 512, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2a_w: (256, 1024, 3, 1, 1) => s4.pathway0_res4.branch2.a.weight: (256, 1024, 3, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2b_bn_rm: (256,) => s4.pathway0_res4.branch2.b_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2b_bn_rm: (512,) => s5.pathway0_res1.branch2.b_bn.running_mean: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2b_bn_b: (512,) => s5.pathway0_res1.branch2.b_bn.bias: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2a_bn_riv: (128,) => s3.pathway0_res1.branch2.a_bn.running_var: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2b_bn_s: (512,) => s5.pathway0_res1.branch2.b_bn.weight: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res4.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2c_bn_riv: (256,) => s2.pathway0_res2.branch2.c_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2c_bn_s: (1024,) => s4.pathway0_res2.branch2.c_bn.weight: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2c_bn_b: (1024,) => s4.pathway0_res2.branch2.c_bn.bias: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2a_w: (64, 256, 3, 1, 1) => s2.pathway0_res2.branch2.a.weight: (64, 256, 3, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2b_bn_riv: (64,) => s2.pathway0_res0.branch2.b_bn.running_var: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2b_bn_riv: (512,) => s5.pathway0_res2.branch2.b_bn.running_var: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch1_bn_riv: (2048,) => s5.pathway0_res0.branch1_bn.running_var: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2a_bn_rm: (64,) => s2.pathway0_res2.branch2.a_bn.running_mean: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2a_bn_s: (256,) => s4.pathway0_res1.branch2.a_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2c_bn_b: (256,) => s2.pathway0_res1.branch2.c_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2b_bn_b: (128,) => s3.pathway0_res2.branch2.b_bn.bias: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2b_bn_s: (128,) => s3.pathway0_res1.branch2.b_bn.weight: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2a_bn_b: (256,) => s4.pathway0_res1.branch2.a_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2c_bn_s: (1024,) => s4.pathway0_res4.branch2.c_bn.weight: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2c_bn_s: (256,) => s2.pathway0_res1.branch2.c_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2b_bn_s: (128,) => s3.pathway0_res2.branch2.b_bn.weight: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2c_bn_s: (2048,) => s5.pathway0_res1.branch2.c_bn.weight: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2c_bn_b: (2048,) => s5.pathway0_res1.branch2.c_bn.bias: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2b_bn_rm: (64,) => s2.pathway0_res1.branch2.b_bn.running_mean: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2b_bn_b: (256,) => s4.pathway0_res1.branch2.b_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2a_w: (64, 64, 3, 1, 1) => s2.pathway0_res0.branch2.a.weight: (64, 64, 3, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2b_bn_s: (256,) => s4.pathway0_res1.branch2.b_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2b_w: (512, 512, 1, 3, 3) => s5.pathway0_res0.branch2.b.weight: (512, 512, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2c_bn_rm: (512,) => s3.pathway0_res1.branch2.c_bn.running_mean: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2a_bn_s: (256,) => s4.pathway0_res4.branch2.a_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2b_bn_s: (256,) => s4.pathway0_res2.branch2.b_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2a_bn_b: (256,) => s4.pathway0_res4.branch2.a_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2b_bn_b: (256,) => s4.pathway0_res2.branch2.b_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2c_bn_rm: (1024,) => s4.pathway0_res3.branch2.c_bn.running_mean: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2c_bn_riv: (1024,) => s4.pathway0_res3.branch2.c_bn.running_var: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2c_bn_rm: (1024,) => s4.pathway0_res2.branch2.c_bn.running_mean: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2a_bn_s: (128,) => s3.pathway0_res0.branch2.a_bn.weight: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2a_bn_b: (128,) => s3.pathway0_res0.branch2.a_bn.bias: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2a_bn_riv: (128,) => s3.pathway0_res3.branch2.a_bn.running_var: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2a_bn_rm: (256,) => s4.pathway0_res5.branch2.a_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2b_bn_riv: (512,) => s5.pathway0_res0.branch2.b_bn.running_var: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2a_bn_b: (128,) => s3.pathway0_res1.branch2.a_bn.bias: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2a_bn_s: (128,) => s3.pathway0_res1.branch2.a_bn.weight: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2a_bn_s: (512,) => s5.pathway0_res1.branch2.a_bn.weight: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2a_bn_riv: (256,) => s4.pathway0_res0.branch2.a_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2a_bn_riv: (512,) => s5.pathway0_res2.branch2.a_bn.running_var: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2a_bn_b: (512,) => s5.pathway0_res1.branch2.a_bn.bias: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2b_bn_riv: (256,) => s4.pathway0_res2.branch2.b_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2b_bn_riv: (256,) => s4.pathway0_res5.branch2.b_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2b_w: (64, 64, 1, 3, 3) => s2.pathway0_res2.branch2.b.weight: (64, 64, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2a_bn_b: (128,) => s3.pathway0_res3.branch2.a_bn.bias: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2a_bn_riv: (64,) => s2.pathway0_res1.branch2.a_bn.running_var: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2a_bn_riv: (256,) => s4.pathway0_res2.branch2.a_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2a_bn_s: (128,) => s3.pathway0_res3.branch2.a_bn.weight: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2b_w: (128, 128, 1, 3, 3) => s3.pathway0_res0.branch2.b.weight: (128, 128, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2a_bn_riv: (256,) => s4.pathway0_res5.branch2.a_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2b_bn_riv: (256,) => s4.pathway0_res0.branch2.b_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2c_w: (2048, 512, 1, 1, 1) => s5.pathway0_res0.branch2.c.weight: (2048, 512, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2a_w: (512, 2048, 1, 1, 1) => s5.pathway0_res2.branch2.a.weight: (512, 2048, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2c_bn_rm: (256,) => s2.pathway0_res0.branch2.c_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch1_w: (2048, 1024, 1, 1, 1) => s5.pathway0_res0.branch1.weight: (2048, 1024, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2c_bn_b: (256,) => s2.pathway0_res2.branch2.c_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2c_bn_b: (1024,) => s4.pathway0_res5.branch2.c_bn.bias: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2b_bn_s: (256,) => s4.pathway0_res3.branch2.b_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2c_bn_riv: (512,) => s3.pathway0_res2.branch2.c_bn.running_var: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2a_w: (512, 1024, 1, 1, 1) => s5.pathway0_res0.branch2.a.weight: (512, 1024, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2c_bn_s: (1024,) => s4.pathway0_res5.branch2.c_bn.weight: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2c_bn_rm: (512,) => s3.pathway0_res3.branch2.c_bn.running_mean: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2b_bn_b: (256,) => s4.pathway0_res3.branch2.b_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2b_bn_rm: (64,) => s2.pathway0_res0.branch2.b_bn.running_mean: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2a_bn_rm: (512,) => s5.pathway0_res1.branch2.a_bn.running_mean: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2b_w: (512, 512, 1, 3, 3) => s5.pathway0_res2.branch2.b.weight: (512, 512, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2c_bn_riv: (1024,) => s4.pathway0_res4.branch2.c_bn.running_var: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2c_bn_b: (1024,) => s4.pathway0_res0.branch2.c_bn.bias: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2b_bn_s: (128,) => s3.pathway0_res3.branch2.b_bn.weight: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2a_w: (256, 1024, 3, 1, 1) => s4.pathway0_res2.branch2.a.weight: (256, 1024, 3, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2c_bn_s: (1024,) => s4.pathway0_res0.branch2.c_bn.weight: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2b_bn_rm: (256,) => s4.pathway0_res3.branch2.b_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2b_bn_b: (128,) => s3.pathway0_res3.branch2.b_bn.bias: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2c_bn_riv: (2048,) => s5.pathway0_res1.branch2.c_bn.running_var: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2a_bn_riv: (256,) => s4.pathway0_res1.branch2.a_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - conv1_w: (64, 3, 5, 7, 7) => s1.pathway0_stem.conv.weight: (64, 3, 5, 7, 7)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch1_bn_riv: (256,) => s2.pathway0_res0.branch1_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2c_bn_rm: (512,) => s3.pathway0_res2.branch2.c_bn.running_mean: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2c_bn_riv: (2048,) => s5.pathway0_res2.branch2.c_bn.running_var: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2c_bn_b: (1024,) => s4.pathway0_res1.branch2.c_bn.bias: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2b_bn_rm: (512,) => s5.pathway0_res0.branch2.b_bn.running_mean: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2b_bn_riv: (256,) => s4.pathway0_res4.branch2.b_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2c_bn_s: (1024,) => s4.pathway0_res1.branch2.c_bn.weight: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2c_bn_riv: (1024,) => s4.pathway0_res1.branch2.c_bn.running_var: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2b_bn_rm: (128,) => s3.pathway0_res1.branch2.b_bn.running_mean: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2c_w: (256, 64, 1, 1, 1) => s2.pathway0_res1.branch2.c.weight: (256, 64, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2a_bn_riv: (64,) => s2.pathway0_res0.branch2.a_bn.running_var: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2a_bn_rm: (512,) => s5.pathway0_res0.branch2.a_bn.running_mean: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2a_bn_b: (512,) => s5.pathway0_res0.branch2.a_bn.bias: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2b_bn_b: (128,) => s3.pathway0_res1.branch2.b_bn.bias: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2a_bn_s: (128,) => s3.pathway0_res2.branch2.a_bn.weight: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2a_bn_s: (512,) => s5.pathway0_res0.branch2.a_bn.weight: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2b_bn_s: (256,) => s4.pathway0_res0.branch2.b_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2a_bn_b: (256,) => s4.pathway0_res2.branch2.a_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2b_bn_s: (512,) => s5.pathway0_res2.branch2.b_bn.weight: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2c_bn_s: (1024,) => s4.pathway0_res3.branch2.c_bn.weight: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2b_bn_s: (512,) => s5.pathway0_res0.branch2.b_bn.weight: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2a_bn_s: (256,) => s4.pathway0_res2.branch2.a_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2b_bn_b: (512,) => s5.pathway0_res2.branch2.b_bn.bias: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2c_bn_b: (1024,) => s4.pathway0_res3.branch2.c_bn.bias: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2b_bn_rm: (128,) => s3.pathway0_res0.branch2.b_bn.running_mean: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2b_bn_b: (512,) => s5.pathway0_res0.branch2.b_bn.bias: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2a_bn_b: (512,) => s5.pathway0_res2.branch2.a_bn.bias: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2c_bn_riv: (512,) => s3.pathway0_res3.branch2.c_bn.running_var: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch1_bn_s: (256,) => s2.pathway0_res0.branch1_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2a_bn_s: (512,) => s5.pathway0_res2.branch2.a_bn.weight: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2c_bn_riv: (256,) => s2.pathway0_res0.branch2.c_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch1_bn_b: (256,) => s2.pathway0_res0.branch1_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2b_bn_rm: (256,) => s4.pathway0_res0.branch2.b_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch1_bn_s: (1024,) => s4.pathway0_res0.branch1_bn.weight: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2a_bn_rm: (256,) => s4.pathway0_res2.branch2.a_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2b_w: (64, 64, 1, 3, 3) => s2.pathway0_res0.branch2.b.weight: (64, 64, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch1_bn_b: (1024,) => s4.pathway0_res0.branch1_bn.bias: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2c_bn_rm: (1024,) => s4.pathway0_res1.branch2.c_bn.running_mean: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2a_bn_rm: (256,) => s4.pathway0_res3.branch2.a_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res2.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch1_bn_b: (512,) => s3.pathway0_res0.branch1_bn.bias: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch1_bn_s: (512,) => s3.pathway0_res0.branch1_bn.weight: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2b_bn_rm: (256,) => s4.pathway0_res1.branch2.b_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2b_bn_b: (256,) => s4.pathway0_res0.branch2.b_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2c_bn_riv: (512,) => s3.pathway0_res1.branch2.c_bn.running_var: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2c_bn_b: (512,) => s3.pathway0_res0.branch2.c_bn.bias: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2a_bn_rm: (128,) => s3.pathway0_res0.branch2.a_bn.running_mean: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2c_bn_s: (2048,) => s5.pathway0_res0.branch2.c_bn.weight: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2b_bn_rm: (256,) => s4.pathway0_res2.branch2.b_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2c_w: (2048, 512, 1, 1, 1) => s5.pathway0_res2.branch2.c.weight: (2048, 512, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2c_bn_s: (512,) => s3.pathway0_res0.branch2.c_bn.weight: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2c_bn_b: (2048,) => s5.pathway0_res0.branch2.c_bn.bias: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2b_bn_riv: (64,) => s2.pathway0_res2.branch2.b_bn.running_var: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2a_bn_riv: (256,) => s4.pathway0_res3.branch2.a_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2c_w: (512, 128, 1, 1, 1) => s3.pathway0_res0.branch2.c.weight: (512, 128, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2c_w: (256, 64, 1, 1, 1) => s2.pathway0_res2.branch2.c.weight: (256, 64, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2b_bn_riv: (128,) => s3.pathway0_res0.branch2.b_bn.running_var: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2a_w: (128, 256, 3, 1, 1) => s3.pathway0_res0.branch2.a.weight: (128, 256, 3, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2a_bn_s: (64,) => s2.pathway0_res0.branch2.a_bn.weight: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2a_bn_b: (256,) => s4.pathway0_res3.branch2.a_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2a_bn_s: (64,) => s2.pathway0_res1.branch2.a_bn.weight: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2c_bn_rm: (512,) => s3.pathway0_res0.branch2.c_bn.running_mean: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2c_bn_b: (512,) => s3.pathway0_res2.branch2.c_bn.bias: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2a_bn_b: (64,) => s2.pathway0_res0.branch2.a_bn.bias: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2b_bn_rm: (256,) => s4.pathway0_res5.branch2.b_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2a_bn_b: (64,) => s2.pathway0_res1.branch2.a_bn.bias: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res2.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2b_w: (128, 128, 1, 3, 3) => s3.pathway0_res1.branch2.b.weight: (128, 128, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2c_bn_s: (512,) => s3.pathway0_res2.branch2.c_bn.weight: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2a_bn_rm: (128,) => s3.pathway0_res1.branch2.a_bn.running_mean: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2c_w: (512, 128, 1, 1, 1) => s3.pathway0_res3.branch2.c.weight: (512, 128, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2b_bn_b: (64,) => s2.pathway0_res0.branch2.b_bn.bias: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2a_bn_s: (256,) => s4.pathway0_res0.branch2.a_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2a_w: (128, 512, 1, 1, 1) => s3.pathway0_res3.branch2.a.weight: (128, 512, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2b_bn_riv: (128,) => s3.pathway0_res1.branch2.b_bn.running_var: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2b_bn_s: (64,) => s2.pathway0_res0.branch2.b_bn.weight: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2a_bn_b: (256,) => s4.pathway0_res0.branch2.a_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2b_bn_rm: (512,) => s5.pathway0_res2.branch2.b_bn.running_mean: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2c_bn_b: (256,) => s2.pathway0_res0.branch2.c_bn.bias: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2b_bn_rm: (128,) => s3.pathway0_res3.branch2.b_bn.running_mean: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2c_bn_s: (256,) => s2.pathway0_res0.branch2.c_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res3.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2b_bn_b: (128,) => s3.pathway0_res0.branch2.b_bn.bias: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch1_w: (256, 64, 1, 1, 1) => s2.pathway0_res0.branch1.weight: (256, 64, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2c_bn_rm: (256,) => s2.pathway0_res1.branch2.c_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2b_bn_s: (128,) => s3.pathway0_res0.branch2.b_bn.weight: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2c_bn_rm: (2048,) => s5.pathway0_res2.branch2.c_bn.running_mean: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res_conv1_bn_riv: (64,) => s1.pathway0_stem.bn.running_var: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2a_bn_riv: (256,) => s4.pathway0_res4.branch2.a_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_1_branch2b_bn_riv: (512,) => s5.pathway0_res1.branch2.b_bn.running_var: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2c_bn_s: (2048,) => s5.pathway0_res2.branch2.c_bn.weight: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch2c_w: (256, 64, 1, 1, 1) => s2.pathway0_res0.branch2.c.weight: (256, 64, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2c_bn_b: (2048,) => s5.pathway0_res2.branch2.c_bn.bias: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2a_bn_s: (256,) => s4.pathway0_res3.branch2.a_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2b_bn_riv: (256,) => s4.pathway0_res3.branch2.b_bn.running_var: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_2_branch2c_bn_riv: (1024,) => s4.pathway0_res2.branch2.c_bn.running_var: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2b_w: (128, 128, 1, 3, 3) => s3.pathway0_res2.branch2.b.weight: (128, 128, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2a_w: (64, 256, 3, 1, 1) => s2.pathway0_res1.branch2.a.weight: (64, 256, 3, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch1_w: (1024, 512, 1, 1, 1) => s4.pathway0_res0.branch1.weight: (1024, 512, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2b_bn_riv: (128,) => s3.pathway0_res2.branch2.b_bn.running_var: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res5.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_0_branch1_bn_rm: (256,) => s2.pathway0_res0.branch1_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2b_bn_s: (64,) => s2.pathway0_res2.branch2.b_bn.weight: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res_conv1_bn_rm: (64,) => s1.pathway0_stem.bn.running_mean: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2b_bn_rm: (128,) => s3.pathway0_res2.branch2.b_bn.running_mean: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2b_bn_b: (64,) => s2.pathway0_res2.branch2.b_bn.bias: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2c_bn_s: (256,) => s2.pathway0_res2.branch2.c_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch1_bn_rm: (1024,) => s4.pathway0_res0.branch1_bn.running_mean: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch1_bn_riv: (1024,) => s4.pathway0_res0.branch1_bn.running_var: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch1_bn_s: (2048,) => s5.pathway0_res0.branch1_bn.weight: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res0.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2b_bn_b: (64,) => s2.pathway0_res1.branch2.b_bn.bias: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch2c_bn_rm: (2048,) => s5.pathway0_res0.branch2.c_bn.running_mean: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_0_branch1_bn_b: (2048,) => s5.pathway0_res0.branch1_bn.bias: (2048,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_0_branch2a_w: (256, 512, 3, 1, 1) => s4.pathway0_res0.branch2.a.weight: (256, 512, 3, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2c_bn_rm: (1024,) => s4.pathway0_res4.branch2.c_bn.running_mean: (1024,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2b_bn_s: (64,) => s2.pathway0_res1.branch2.b_bn.weight: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch1_bn_riv: (512,) => s3.pathway0_res0.branch1_bn.running_var: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2a_bn_rm: (256,) => s4.pathway0_res1.branch2.a_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_0_branch2c_bn_riv: (512,) => s3.pathway0_res0.branch2.c_bn.running_var: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2a_w: (128, 512, 1, 1, 1) => s3.pathway0_res1.branch2.a.weight: (128, 512, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2b_w: (64, 64, 1, 3, 3) => s2.pathway0_res1.branch2.b.weight: (64, 64, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_1_branch2c_w: (512, 128, 1, 1, 1) => s3.pathway0_res1.branch2.c.weight: (512, 128, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_2_branch2a_bn_riv: (128,) => s3.pathway0_res2.branch2.a_bn.running_var: (128,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_2_branch2c_bn_rm: (256,) => s2.pathway0_res2.branch2.c_bn.running_mean: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  204 - WARNING - !! pred_b: (400,) does not match head.projection.bias: (51,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res3_3_branch2b_w: (128, 128, 1, 3, 3) => s3.pathway0_res3.branch2.b.weight: (128, 128, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res2_1_branch2b_bn_riv: (64,) => s2.pathway0_res1.branch2.b_bn.running_var: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  204 - WARNING - !! pred_w: (400, 2048) does not match head.projection.weight: (51, 2048)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2a_w: (256, 1024, 1, 1, 1) => s4.pathway0_res3.branch2.a.weight: (256, 1024, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res1.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res3.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2a_bn_s: (256,) => s4.pathway0_res5.branch2.a_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2a_bn_b: (256,) => s4.pathway0_res5.branch2.a_bn.bias: (256,)\n",
      "pyvideoai.train_multiprocess:  220 - INFO - cfg.criterion not defined. Using CrossEntropyLoss()\n",
      "pyvideoai.train_multiprocess:  248 - INFO - ReduceLROnPlateau scheduler is selected. The `scheduler.step(val_loss)` function will be called at the end of epoch (after validation), but not every iteration.\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 0/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 3.8874 - loss: 3.8073 - batch_acc: 0.1250 - acc: 0.1085        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 3.8073 - acc: 0.1085                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.9147 - val_acc: 0.3732        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.9147 - val_acc: 0.3732\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0000.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 1/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 3.5042 - loss: 3.4460 - batch_acc: 0.2500 - acc: 0.3310        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 3.4460 - acc: 0.3310                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.8353 - val_acc: 0.4196        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.8353 - val_acc: 0.4196\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0001.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0000.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 2/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 2.3087 - loss: 3.0303 - batch_acc: 0.8750 - acc: 0.4268        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 3.0303 - acc: 0.4268                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.7516 - val_acc: 0.4928        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.7516 - val_acc: 0.4928\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0002.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0001.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 3/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.9789 - loss: 2.6374 - batch_acc: 0.7500 - acc: 0.4728        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 2.6374 - acc: 0.4728                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.6776 - val_acc: 0.5340        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.6776 - val_acc: 0.5340\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0003.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0002.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 4/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.9224 - loss: 2.3379 - batch_acc: 0.5000 - acc: 0.5191        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 2.3379 - acc: 0.5191                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.6163 - val_acc: 0.5588        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.6163 - val_acc: 0.5588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.6214 - val_acc: 0.5582        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.6214 - val_acc: 0.5582 - val_vid_acc_top1: 0.5712 - val_vid_acc_top5: 0.8595\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0004.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0003.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 5/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 2.1446 - loss: 2.1000 - batch_acc: 0.6250 - acc: 0.5597        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 2.1000 - acc: 0.5597                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.5732 - val_acc: 0.5895        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.5732 - val_acc: 0.5895\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0005.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0004.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 6/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 2.5741 - loss: 1.9209 - batch_acc: 0.3750 - acc: 0.5788        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.9209 - acc: 0.5788                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.5446 - val_acc: 0.6248        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.5446 - val_acc: 0.6248\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0006.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0005.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 7/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.0247 - loss: 1.7621 - batch_acc: 0.7500 - acc: 0.6057        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.7621 - acc: 0.6057                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.5182 - val_acc: 0.6431        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.5182 - val_acc: 0.6431\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0007.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0006.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 8/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.4834 - loss: 1.6438 - batch_acc: 0.6250 - acc: 0.6272        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.6438 - acc: 0.6272                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.4965 - val_acc: 0.6627        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.4965 - val_acc: 0.6627\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0008.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0007.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 9/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.8923 - loss: 1.5573 - batch_acc: 1.0000 - acc: 0.6421        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.5573 - acc: 0.6421                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.4748 - val_acc: 0.6654        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.4748 - val_acc: 0.6654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.4804 - val_acc: 0.6601        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.4804 - val_acc: 0.6601 - val_vid_acc_top1: 0.6765 - val_vid_acc_top5: 0.9118\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0009.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0008.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 10/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.1748 - loss: 1.4623 - batch_acc: 0.8750 - acc: 0.6603        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.4623 - acc: 0.6603                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.4599 - val_acc: 0.6595        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.4599 - val_acc: 0.6595\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0010.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 11/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.6865 - loss: 1.4015 - batch_acc: 0.6250 - acc: 0.6679        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.4015 - acc: 0.6679                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.4338 - val_acc: 0.6765        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.4338 - val_acc: 0.6765\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0011.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0010.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 12/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.4621 - loss: 1.3347 - batch_acc: 0.5000 - acc: 0.6841        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.3347 - acc: 0.6841                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.4363 - val_acc: 0.6837        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.4363 - val_acc: 0.6837\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0012.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0011.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 13/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.7931 - loss: 1.2776 - batch_acc: 0.7500 - acc: 0.6881        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.2776 - acc: 0.6881                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.4153 - val_acc: 0.6830        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.4153 - val_acc: 0.6830\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0013.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 14/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.0173 - loss: 1.2382 - batch_acc: 0.8750 - acc: 0.6956        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.2382 - acc: 0.6956                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.4065 - val_acc: 0.6843        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.4065 - val_acc: 0.6843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.4122 - val_acc: 0.6833        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.4122 - val_acc: 0.6833 - val_vid_acc_top1: 0.7039 - val_vid_acc_top5: 0.9222\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0014.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0013.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 15/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.2573 - loss: 1.1823 - batch_acc: 0.7500 - acc: 0.7052        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.1823 - acc: 0.7052                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3969 - val_acc: 0.6889        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3969 - val_acc: 0.6889\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0015.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0014.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 16/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.0895 - loss: 1.1441 - batch_acc: 0.6250 - acc: 0.7077        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.1441 - acc: 0.7077                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3890 - val_acc: 0.6908        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3890 - val_acc: 0.6908\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0016.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0015.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 17/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.1057 - loss: 1.1120 - batch_acc: 0.7500 - acc: 0.7178        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.1120 - acc: 0.7178                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3865 - val_acc: 0.6902        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3865 - val_acc: 0.6902\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0017.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 18/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.8351 - loss: 1.0695 - batch_acc: 0.3750 - acc: 0.7323        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.0695 - acc: 0.7323                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3795 - val_acc: 0.6876        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3795 - val_acc: 0.6876\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0018.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0017.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 19/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.4866 - loss: 1.0676 - batch_acc: 0.6250 - acc: 0.7223        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 1.0676 - acc: 0.7223                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3741 - val_acc: 0.6922        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3741 - val_acc: 0.6922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3790 - val_acc: 0.6928        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3790 - val_acc: 0.6928 - val_vid_acc_top1: 0.7131 - val_vid_acc_top5: 0.9333\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0019.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0018.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 20/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.4410 - loss: 1.0555 - batch_acc: 0.7500 - acc: 0.7270        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 95s - lr: 0.00010000 - loss: 1.0555 - acc: 0.7270                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3667 - val_acc: 0.6974        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3667 - val_acc: 0.6974\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0020.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0019.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 21/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.9957 - loss: 0.9979 - batch_acc: 0.7500 - acc: 0.7475        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.9979 - acc: 0.7475                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3609 - val_acc: 0.7000        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3609 - val_acc: 0.7000\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0021.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0020.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 22/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 2.0847 - loss: 0.9697 - batch_acc: 0.3750 - acc: 0.7514        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.9697 - acc: 0.7514                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3539 - val_acc: 0.6941        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3539 - val_acc: 0.6941\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0022.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 23/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.1064 - loss: 0.9540 - batch_acc: 0.6250 - acc: 0.7480        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.9540 - acc: 0.7480                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3610 - val_acc: 0.6993        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3610 - val_acc: 0.6993\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0023.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0022.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 24/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.1071 - loss: 0.9382 - batch_acc: 0.7500 - acc: 0.7562        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.9382 - acc: 0.7562                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3564 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3564 - val_acc: 0.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3616 - val_acc: 0.7009        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3616 - val_acc: 0.7009 - val_vid_acc_top1: 0.7222 - val_vid_acc_top5: 0.9366\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0024.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0023.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 25/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.0092 - loss: 0.9178 - batch_acc: 0.7500 - acc: 0.7604        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.9178 - acc: 0.7604                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3489 - val_acc: 0.7007        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3489 - val_acc: 0.7007\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0025.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 26/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.6743 - loss: 0.8734 - batch_acc: 0.8750 - acc: 0.7727        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.8734 - acc: 0.7727                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3473 - val_acc: 0.6974        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3473 - val_acc: 0.6974\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0026.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0025.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 27/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.8931 - loss: 0.8792 - batch_acc: 1.0000 - acc: 0.7663        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.8792 - acc: 0.7663                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3444 - val_acc: 0.6961        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3444 - val_acc: 0.6961\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0027.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0026.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 28/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.6331 - loss: 0.8553 - batch_acc: 0.8750 - acc: 0.7738        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.8553 - acc: 0.7738                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3417 - val_acc: 0.6993        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3417 - val_acc: 0.6993\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0028.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0027.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 29/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.9639 - loss: 0.8356 - batch_acc: 0.7500 - acc: 0.7738        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.8356 - acc: 0.7738                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3445 - val_acc: 0.7000        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3445 - val_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3487 - val_acc: 0.7024        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3487 - val_acc: 0.7024 - val_vid_acc_top1: 0.7203 - val_vid_acc_top5: 0.9359\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0029.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0028.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 30/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.6083 - loss: 0.8266 - batch_acc: 0.8750 - acc: 0.7786        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.8266 - acc: 0.7786                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3368 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3368 - val_acc: 0.7026\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0030.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0029.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 31/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.8300 - loss: 0.7922 - batch_acc: 0.7500 - acc: 0.7859        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.7922 - acc: 0.7859                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3353 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3353 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0031.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 32/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.2860 - loss: 0.7778 - batch_acc: 0.7500 - acc: 0.7971        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.7778 - acc: 0.7971                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3386 - val_acc: 0.7059        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3386 - val_acc: 0.7059\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0032.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0031.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 33/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.6379 - loss: 0.7938 - batch_acc: 0.7500 - acc: 0.7878        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.7938 - acc: 0.7878                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3337 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3337 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0033.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 34/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.3728 - loss: 0.7681 - batch_acc: 0.8750 - acc: 0.7940        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.7681 - acc: 0.7940                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3291 - val_acc: 0.7065        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3291 - val_acc: 0.7065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3336 - val_acc: 0.7051        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3336 - val_acc: 0.7051 - val_vid_acc_top1: 0.7281 - val_vid_acc_top5: 0.9386\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0034.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0033.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 35/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.5471 - loss: 0.7706 - batch_acc: 0.8750 - acc: 0.7951        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.7706 - acc: 0.7951                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3331 - val_acc: 0.7013        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3331 - val_acc: 0.7013\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0035.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 36/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.9798 - loss: 0.7491 - batch_acc: 0.7500 - acc: 0.8004        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.7491 - acc: 0.8004                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3278 - val_acc: 0.7065        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3278 - val_acc: 0.7065\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0036.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0035.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 37/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.7140 - loss: 0.7217 - batch_acc: 0.7500 - acc: 0.8139        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.7217 - acc: 0.8139                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3338 - val_acc: 0.7085        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3338 - val_acc: 0.7085\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0037.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0036.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 38/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.5061 - loss: 0.7255 - batch_acc: 0.8750 - acc: 0.8027        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.7255 - acc: 0.8027                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3259 - val_acc: 0.7078        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3259 - val_acc: 0.7078\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0038.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 39/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.4402 - loss: 0.7001 - batch_acc: 0.6250 - acc: 0.8066        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.7001 - acc: 0.8066                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3300 - val_acc: 0.7033        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3300 - val_acc: 0.7033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3336 - val_acc: 0.7041        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3336 - val_acc: 0.7041 - val_vid_acc_top1: 0.7261 - val_vid_acc_top5: 0.9399\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0039.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0038.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 40/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.7949 - loss: 0.6917 - batch_acc: 0.7500 - acc: 0.8153        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.6917 - acc: 0.8153                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3327 - val_acc: 0.7065        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3327 - val_acc: 0.7065\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0040.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0039.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 41/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.8939 - loss: 0.6725 - batch_acc: 0.8750 - acc: 0.8217        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.6725 - acc: 0.8217                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3237 - val_acc: 0.7085        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3237 - val_acc: 0.7085\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0041.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0040.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 42/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.3912 - loss: 0.6675 - batch_acc: 0.8750 - acc: 0.8246        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.6675 - acc: 0.8246                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3190 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3190 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0042.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 43/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.7911 - loss: 0.6408 - batch_acc: 0.5000 - acc: 0.8296        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.6408 - acc: 0.8296                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3216 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3216 - val_acc: 0.7046\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0043.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0042.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 44/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.0851 - loss: 0.6477 - batch_acc: 0.7500 - acc: 0.8229        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.6477 - acc: 0.8229                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3209 - val_acc: 0.7065        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3209 - val_acc: 0.7065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3242 - val_acc: 0.7068        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3242 - val_acc: 0.7068 - val_vid_acc_top1: 0.7248 - val_vid_acc_top5: 0.9399\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0044.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0043.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 45/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.2686 - loss: 0.6253 - batch_acc: 0.6250 - acc: 0.8397        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.6253 - acc: 0.8397                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3232 - val_acc: 0.7065        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3232 - val_acc: 0.7065\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0045.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0044.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 46/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 0.3106 - loss: 0.6233 - batch_acc: 1.0000 - acc: 0.8391        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.6233 - acc: 0.8391                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3229 - val_acc: 0.6980        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3229 - val_acc: 0.6980\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0046.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0045.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 47/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 1.4136 - loss: 0.6241 - batch_acc: 0.6250 - acc: 0.8296        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 0.6241 - acc: 0.8296                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3205 - val_acc: 0.6974        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torch.optim.lr_scheduler:   16 - INFO - Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3205 - val_acc: 0.6974\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0047.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0046.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 48/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00001000 - batch_loss: 0.5647 - loss: 0.6088 - batch_acc: 0.7500 - acc: 0.8355        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00001000 - loss: 0.6088 - acc: 0.8355                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3228 - val_acc: 0.6954        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3228 - val_acc: 0.6954\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0048.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0047.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 49/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00001000 - batch_loss: 0.4253 - loss: 0.5960 - batch_acc: 1.0000 - acc: 0.8414        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00001000 - loss: 0.5960 - acc: 0.8414                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3191 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3191 - val_acc: 0.7039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3223 - val_acc: 0.7024        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3223 - val_acc: 0.7024 - val_vid_acc_top1: 0.7268 - val_vid_acc_top5: 0.9399\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0049.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0048.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 50/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00001000 - batch_loss: 0.6076 - loss: 0.5952 - batch_acc: 0.8750 - acc: 0.8447        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00001000 - loss: 0.5952 - acc: 0.8447                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3201 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3201 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0050.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0049.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 51/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00001000 - batch_loss: 0.5375 - loss: 0.5884 - batch_acc: 1.0000 - acc: 0.8442        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00001000 - loss: 0.5884 - acc: 0.8442                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3212 - val_acc: 0.6993        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3212 - val_acc: 0.6993\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0051.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0050.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 52/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00001000 - batch_loss: 0.0584 - loss: 0.6074 - batch_acc: 1.0000 - acc: 0.8400        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00001000 - loss: 0.6074 - acc: 0.8400                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3192 - val_acc: 0.7000        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torch.optim.lr_scheduler:   16 - INFO - Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3192 - val_acc: 0.7000\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0052.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0051.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 53/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000100 - batch_loss: 0.7557 - loss: 0.5934 - batch_acc: 0.8750 - acc: 0.8400        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000100 - loss: 0.5934 - acc: 0.8400                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3170 - val_acc: 0.7000        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3170 - val_acc: 0.7000\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0053.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0052.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 54/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000100 - batch_loss: 1.4657 - loss: 0.5943 - batch_acc: 0.6250 - acc: 0.8439        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000100 - loss: 0.5943 - acc: 0.8439                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3212 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3212 - val_acc: 0.7039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3248 - val_acc: 0.7048        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3248 - val_acc: 0.7048 - val_vid_acc_top1: 0.7248 - val_vid_acc_top5: 0.9425\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0054.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0053.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 55/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000100 - batch_loss: 0.4903 - loss: 0.5939 - batch_acc: 0.8750 - acc: 0.8453        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000100 - loss: 0.5939 - acc: 0.8453                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3215 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3215 - val_acc: 0.7046\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0055.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0054.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 56/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000100 - batch_loss: 0.3326 - loss: 0.6124 - batch_acc: 0.7500 - acc: 0.8341        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000100 - loss: 0.6124 - acc: 0.8341                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3237 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3237 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0056.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0055.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 57/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000100 - batch_loss: 0.5072 - loss: 0.6044 - batch_acc: 0.8750 - acc: 0.8433        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000100 - loss: 0.6044 - acc: 0.8433                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3208 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3208 - val_acc: 0.7026\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0057.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0056.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 58/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000100 - batch_loss: 0.7399 - loss: 0.5905 - batch_acc: 0.8750 - acc: 0.8473        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000100 - loss: 0.5905 - acc: 0.8473                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3212 - val_acc: 0.7013        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torch.optim.lr_scheduler:   16 - INFO - Epoch    70: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3212 - val_acc: 0.7013\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0058.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0057.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 59/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000010 - batch_loss: 0.4075 - loss: 0.5858 - batch_acc: 0.8750 - acc: 0.8492        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000010 - loss: 0.5858 - acc: 0.8492                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3227 - val_acc: 0.7059        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3227 - val_acc: 0.7059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3244 - val_acc: 0.7063        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3244 - val_acc: 0.7063 - val_vid_acc_top1: 0.7301 - val_vid_acc_top5: 0.9425\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0059.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0058.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 60/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000010 - batch_loss: 0.7323 - loss: 0.5882 - batch_acc: 0.7500 - acc: 0.8473        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 95s - lr: 0.00000010 - loss: 0.5882 - acc: 0.8473                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3206 - val_acc: 0.7078        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3206 - val_acc: 0.7078\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0060.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0059.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 61/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000010 - batch_loss: 0.5106 - loss: 0.6074 - batch_acc: 1.0000 - acc: 0.8383        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000010 - loss: 0.6074 - acc: 0.8383                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3213 - val_acc: 0.6987        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3213 - val_acc: 0.6987\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0061.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0060.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 62/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000010 - batch_loss: 0.2850 - loss: 0.6007 - batch_acc: 1.0000 - acc: 0.8439        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000010 - loss: 0.6007 - acc: 0.8439                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3242 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3242 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0062.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0061.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 63/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000010 - batch_loss: 0.9271 - loss: 0.5853 - batch_acc: 0.8750 - acc: 0.8419        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000010 - loss: 0.5853 - acc: 0.8419                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3309 - val_acc: 0.7039        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torch.optim.lr_scheduler:   16 - INFO - Epoch    76: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3309 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0063.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0062.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 64/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9381 - loss: 0.5967 - batch_acc: 0.6250 - acc: 0.8487        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5967 - acc: 0.8487                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3213 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3213 - val_acc: 0.7046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3248 - val_acc: 0.7084        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3248 - val_acc: 0.7084 - val_vid_acc_top1: 0.7307 - val_vid_acc_top5: 0.9386\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0064.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0063.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 65/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.3107 - loss: 0.5889 - batch_acc: 0.5000 - acc: 0.8489        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5889 - acc: 0.8489                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3237 - val_acc: 0.6980        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3237 - val_acc: 0.6980\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0065.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0064.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 66/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.1378 - loss: 0.5998 - batch_acc: 1.0000 - acc: 0.8366        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5998 - acc: 0.8366                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3204 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3204 - val_acc: 0.7026\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0066.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0065.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 67/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9831 - loss: 0.5945 - batch_acc: 0.6250 - acc: 0.8459        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5945 - acc: 0.8459                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3220 - val_acc: 0.6987        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3220 - val_acc: 0.6987\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0067.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0066.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 68/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.3072 - loss: 0.6118 - batch_acc: 0.8750 - acc: 0.8344        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6118 - acc: 0.8344                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3202 - val_acc: 0.7007        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3202 - val_acc: 0.7007\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0068.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0067.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 69/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.7962 - loss: 0.6069 - batch_acc: 0.7500 - acc: 0.8346        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6069 - acc: 0.8346                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3224 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3224 - val_acc: 0.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3250 - val_acc: 0.7068        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3250 - val_acc: 0.7068 - val_vid_acc_top1: 0.7307 - val_vid_acc_top5: 0.9405\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0069.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0068.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 70/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9098 - loss: 0.6077 - batch_acc: 0.7500 - acc: 0.8374        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 95s - lr: 0.00000001 - loss: 0.6077 - acc: 0.8374                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3194 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3194 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0070.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0069.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 71/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.8109 - loss: 0.6043 - batch_acc: 0.8750 - acc: 0.8372        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6043 - acc: 0.8372                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3182 - val_acc: 0.7072        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3182 - val_acc: 0.7072\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0071.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0070.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 72/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.3911 - loss: 0.6008 - batch_acc: 0.6250 - acc: 0.8388        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6008 - acc: 0.8388                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3229 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3229 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0072.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0071.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 73/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.8925 - loss: 0.5937 - batch_acc: 0.7500 - acc: 0.8430        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5937 - acc: 0.8430                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3192 - val_acc: 0.7000        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3192 - val_acc: 0.7000\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0073.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0072.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 74/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.7304 - loss: 0.5988 - batch_acc: 0.7500 - acc: 0.8475        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5988 - acc: 0.8475                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3209 - val_acc: 0.7065        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3209 - val_acc: 0.7065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3242 - val_acc: 0.7071        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3242 - val_acc: 0.7071 - val_vid_acc_top1: 0.7242 - val_vid_acc_top5: 0.9392\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0074.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0073.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 75/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4877 - loss: 0.5827 - batch_acc: 1.0000 - acc: 0.8489        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5827 - acc: 0.8489                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3183 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3183 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0075.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0074.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 76/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5338 - loss: 0.5995 - batch_acc: 0.7500 - acc: 0.8391        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5995 - acc: 0.8391                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3197 - val_acc: 0.7059        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3197 - val_acc: 0.7059\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0076.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0075.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 77/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.8394 - loss: 0.6008 - batch_acc: 0.8750 - acc: 0.8436        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6008 - acc: 0.8436                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3183 - val_acc: 0.7052        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3183 - val_acc: 0.7052\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0077.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0076.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 78/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9178 - loss: 0.5857 - batch_acc: 0.7500 - acc: 0.8481        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5857 - acc: 0.8481                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3239 - val_acc: 0.7013        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3239 - val_acc: 0.7013\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0078.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0077.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 79/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.1290 - loss: 0.5965 - batch_acc: 0.6250 - acc: 0.8492        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5965 - acc: 0.8492                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3235 - val_acc: 0.7033        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3235 - val_acc: 0.7033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3262 - val_acc: 0.7033        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3262 - val_acc: 0.7033 - val_vid_acc_top1: 0.7261 - val_vid_acc_top5: 0.9399\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0079.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0078.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 80/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.0340 - loss: 0.5944 - batch_acc: 0.7500 - acc: 0.8459        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5944 - acc: 0.8459                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3205 - val_acc: 0.6980        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3205 - val_acc: 0.6980\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0080.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0079.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 81/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.0763 - loss: 0.5803 - batch_acc: 1.0000 - acc: 0.8473        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5803 - acc: 0.8473                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3211 - val_acc: 0.6993        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3211 - val_acc: 0.6993\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0081.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0080.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 82/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9557 - loss: 0.6174 - batch_acc: 0.7500 - acc: 0.8341        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6174 - acc: 0.8341                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3194 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3194 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0082.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0081.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 83/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4809 - loss: 0.5932 - batch_acc: 0.8750 - acc: 0.8456        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5932 - acc: 0.8456                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3225 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3225 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0083.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0082.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 84/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.6333 - loss: 0.6036 - batch_acc: 0.8750 - acc: 0.8411        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6036 - acc: 0.8411                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3184 - val_acc: 0.7078        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3184 - val_acc: 0.7078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3202 - val_acc: 0.7114        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3202 - val_acc: 0.7114 - val_vid_acc_top1: 0.7320 - val_vid_acc_top5: 0.9405\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0084.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0083.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 85/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.8376 - loss: 0.5944 - batch_acc: 0.8750 - acc: 0.8464        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5944 - acc: 0.8464                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3238 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3238 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0085.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0084.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 86/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.2305 - loss: 0.5845 - batch_acc: 1.0000 - acc: 0.8456        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5845 - acc: 0.8456                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3197 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3197 - val_acc: 0.7026\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0086.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0085.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 87/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.1432 - loss: 0.6019 - batch_acc: 0.7500 - acc: 0.8332        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6019 - acc: 0.8332                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3225 - val_acc: 0.7052        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3225 - val_acc: 0.7052\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0087.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0086.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 88/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5862 - loss: 0.5880 - batch_acc: 0.8750 - acc: 0.8489        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5880 - acc: 0.8489                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3205 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3205 - val_acc: 0.7026\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0088.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0087.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 89/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.1097 - loss: 0.5854 - batch_acc: 1.0000 - acc: 0.8400        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5854 - acc: 0.8400                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3200 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3200 - val_acc: 0.7039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3231 - val_acc: 0.7054        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3231 - val_acc: 0.7054 - val_vid_acc_top1: 0.7255 - val_vid_acc_top5: 0.9399\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0089.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0088.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 90/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5100 - loss: 0.6096 - batch_acc: 0.8750 - acc: 0.8374        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6096 - acc: 0.8374                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3231 - val_acc: 0.6987        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3231 - val_acc: 0.6987\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0090.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0089.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 91/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.3481 - loss: 0.6026 - batch_acc: 0.8750 - acc: 0.8391        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6026 - acc: 0.8391                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3209 - val_acc: 0.7059        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3209 - val_acc: 0.7059\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0091.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0090.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 92/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.6989 - loss: 0.6064 - batch_acc: 0.8750 - acc: 0.8380        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6064 - acc: 0.8380                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3185 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3185 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0092.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0091.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 93/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.3694 - loss: 0.5949 - batch_acc: 1.0000 - acc: 0.8405        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5949 - acc: 0.8405                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3216 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3216 - val_acc: 0.7046\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0093.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0092.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 94/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.3702 - loss: 0.5908 - batch_acc: 0.8750 - acc: 0.8416        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5908 - acc: 0.8416                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3211 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3211 - val_acc: 0.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3254 - val_acc: 0.7042        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3254 - val_acc: 0.7042 - val_vid_acc_top1: 0.7294 - val_vid_acc_top5: 0.9425\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0094.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0093.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 95/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4365 - loss: 0.5888 - batch_acc: 0.8750 - acc: 0.8475        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5888 - acc: 0.8475                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3194 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3194 - val_acc: 0.7026\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0095.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0094.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 96/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.0890 - loss: 0.5887 - batch_acc: 1.0000 - acc: 0.8439        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5887 - acc: 0.8439                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3194 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3194 - val_acc: 0.7046\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0096.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0095.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 97/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.2389 - loss: 0.6019 - batch_acc: 0.6250 - acc: 0.8456        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6019 - acc: 0.8456                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3184 - val_acc: 0.7033        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3184 - val_acc: 0.7033\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0097.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0096.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 98/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.1730 - loss: 0.5969 - batch_acc: 0.6250 - acc: 0.8461        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5969 - acc: 0.8461                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3158 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3158 - val_acc: 0.7046\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0098.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0097.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 99/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.8652 - loss: 0.5864 - batch_acc: 0.7500 - acc: 0.8422        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5864 - acc: 0.8422                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3208 - val_acc: 0.7033        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3208 - val_acc: 0.7033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3233 - val_acc: 0.7082        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3233 - val_acc: 0.7082 - val_vid_acc_top1: 0.7301 - val_vid_acc_top5: 0.9405\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0099.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0098.pth\n",
      "pyvideoai.train_multiprocess:  455 - SUCCESS - Finished training\n"
     ]
    }
   ],
   "source": [
    "# If you have more than 1 GPUs, go ahead and change CUDA_VISIBLE_DEVICES to the comma-separated GPU indices, and --local_world_size to the number of GPUs.\n",
    "# This will increase the batch size (1 GPU=size 8, 2 GPU=size 16, ...) and speed up the learning.\n",
    "# However, Jupyter Notebook doesn't seem to support stdout printing with multiple processes. Try using normal shell.\n",
    "\n",
    "# -e sets the number of epochs.\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%run \"$PYVIDEOAI_DIR/tools/run_train.py\" --local_world_size 1 -D hmdb -M i3d_resnet50 -N crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1 -e 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ff61a-551c-40b7-9928-a80f91fdb7fc",
   "metadata": {},
   "source": [
    "### Multicrop evaluation\n",
    "Since one clip is too short to see the entire video, it will perform multicrop (1 centre crop, 5 temporal crops) evaluation every 5 epochs (`-t 5`).  \n",
    "FYI, most Facebook AI Research papers (e.g. SlowFast) use 3 spatial crops and 10 temporal crops (totalling 30 clips per video).  \n",
    "To change the frequency, add `-t EPOCHS`.  \n",
    "To skip the multicrop evaluation, add `-t -1`.  \n",
    "To change the number of spatial and temporal crops, edit `test_num_spatial_crops` and `test_num_ensemble_views` in `exp_configs/hmdb/i3d_resnet50-crop224_lr001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1.py`.  \n",
    "Or, you can change the dataloader by changing `def get_torch_dataset(split)` function in the config file.\n",
    "\n",
    "### Saving model weights\n",
    "It will only keep the models with peak validation accuracy, plus the last epoch. That's why you see that it's removing the previous model when it's not better than current.  \n",
    "This is due to the command line argument `--save_mode last_and_peaks` which is set by default.  \n",
    "Use `--save_mode all` in order to keep checkpoints from every epoch.  \n",
    "\n",
    "### Experiment output structure\n",
    "TODO: explain experiment output structure\n",
    "\n",
    "### Visualisation\n",
    "\n",
    "If you've set up the Telegram bot, it will report the training stats (and even get notifications of errors running the script) like this:\n",
    "\n",
    "TODO: Add Telegram and TensorBoard example plots.\n",
    "\n",
    "You can use TensorBoard in `data/experiments/hmdb/i3d_resnet/crop224_lr001_batch8_8x8_largejit_steplr_1scrop5tcrop_split1/tensorboard_runs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb811fc-9c96-4ec5-ae1f-31b2d021451b",
   "metadata": {},
   "source": [
    "## Resume training\n",
    "\n",
    "### From the last checkpoint\n",
    "By simply adding `-l -1`, it will resume from the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "274ae7ad-65e1-439d-a998-7c7d368e7a1b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment_utils.experiment_builder:  103 - INFO - Telegram bot initialised with keys in /home/kiyoon/project/PyVideoAI/tools/key.ini and using the bot number 0\n",
      "pyvideoai.train_multiprocess:  121 - INFO - git hash: 48a5d20dc2e17fcd7b4343cc562f043b9839d6e2\n",
      "pyvideoai.train_multiprocess:  125 - INFO - args: {\n",
      "    \"local_world_size\": 1,\n",
      "    \"shard_id\": 0,\n",
      "    \"num_shards\": 1,\n",
      "    \"init_method\": \"tcp://localhost:19999\",\n",
      "    \"backend\": \"nccl\",\n",
      "    \"num_epochs\": 200,\n",
      "    \"experiment_root\": \"/home/kiyoon/project/PyVideoAI/data/experiments\",\n",
      "    \"dataset\": \"hmdb\",\n",
      "    \"model\": \"i3d_resnet50\",\n",
      "    \"experiment_name\": \"crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1\",\n",
      "    \"dataset_channel\": null,\n",
      "    \"model_channel\": null,\n",
      "    \"experiment_channel\": null,\n",
      "    \"save_mode\": \"last_and_peaks\",\n",
      "    \"load_epoch\": -1,\n",
      "    \"seed\": 12,\n",
      "    \"multi_crop_val_freq\": 5,\n",
      "    \"telegram_post_freq\": 5,\n",
      "    \"telegram_bot_idx\": 0,\n",
      "    \"dataloader_num_workers\": 4\n",
      "}\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset train...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 3570) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/train1.csv\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset test...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 1530) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/test1.csv\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset test...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 7650) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/test1.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.utils.misc:   52 - INFO - Model:\n",
      "ResNetModel(\n",
      "  (s1): VideoModelStem(\n",
      "    (pathway0_stem): ResNetBasicStem(\n",
      "      (conv): Conv3d(3, 64, kernel_size=[5, 7, 7], stride=[1, 2, 2], padding=[2, 3, 3], bias=False)\n",
      "      (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (pool_layer): MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (s2): ResStage(\n",
      "    (pathway0_res0): ResBlock(\n",
      "      (branch1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=[1, 1, 1], bias=False)\n",
      "      (branch1_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(64, 64, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(64, 64, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(64, 256, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res1): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(256, 64, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(64, 64, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(64, 256, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res2): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(256, 64, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(64, 64, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(64, 256, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pathway0_pool): MaxPool3d(kernel_size=[2, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=1, ceil_mode=False)\n",
      "  (s3): ResStage(\n",
      "    (pathway0_res0): ResBlock(\n",
      "      (branch1): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=[1, 2, 2], bias=False)\n",
      "      (branch1_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(256, 128, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(128, 128, kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(128, 512, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res1): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(512, 128, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(128, 128, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(128, 512, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res2): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(512, 128, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(128, 128, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(128, 512, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res3): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(512, 128, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(128, 128, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(128, 512, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (s4): ResStage(\n",
      "    (pathway0_res0): ResBlock(\n",
      "      (branch1): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=[1, 2, 2], bias=False)\n",
      "      (branch1_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(512, 256, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res1): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res2): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res3): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res4): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res5): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (s5): ResStage(\n",
      "    (pathway0_res0): ResBlock(\n",
      "      (branch1): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=[1, 2, 2], bias=False)\n",
      "      (branch1_bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 512, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(512, 512, kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(512, 2048, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res1): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(2048, 512, kernel_size=[3, 1, 1], stride=[1, 1, 1], padding=[1, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(512, 512, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(512, 2048, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res2): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(2048, 512, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (a_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(512, 512, kernel_size=[1, 3, 3], stride=[1, 1, 1], padding=[0, 1, 1], dilation=[1, 1, 1], bias=False)\n",
      "        (b_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(512, 2048, kernel_size=[1, 1, 1], stride=[1, 1, 1], padding=[0, 0, 0], bias=False)\n",
      "        (c_bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (head): ResNetBasicHead(\n",
      "    (pathway0_avgpool): AvgPool3d(kernel_size=[4, 7, 7], stride=1, padding=0)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (projection): Linear(in_features=2048, out_features=51, bias=True)\n",
      "    (act): Softmax(dim=4)\n",
      "  )\n",
      ")\n",
      "pyvideoai.utils.misc:   53 - INFO - Params: 27,328,371\n",
      "pyvideoai.utils.misc:   54 - INFO - Mem: 4,840.8818359375 MiB\n",
      "pyvideoai.utils.misc:   55 - INFO - nvidia-smi\n",
      "pyvideoai.utils.misc:   56 - INFO - Wed Jun 16 00:02:21 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3090    Off  | 00000000:23:00.0 Off |                  N/A |\n",
      "|  0%   34C    P2    86W / 370W |   6365MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 3090    Off  | 00000000:2D:00.0 Off |                  N/A |\n",
      "| 38%   46C    P0    40W / 370W |      0MiB / 24251MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1353154      C   ...3/envs/videoai/bin/python     6363MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "pyvideoai.utils.loader:   93 - INFO - Loading weights: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0099.pth\n",
      "pyvideoai.train_multiprocess:  220 - INFO - cfg.criterion not defined. Using CrossEntropyLoss()\n",
      "pyvideoai.train_multiprocess:  236 - INFO - cfg.load_optimiser_state not found. By default, it will try to load.\n",
      "pyvideoai.train_multiprocess:  240 - INFO - Loading optimiser state from the checkpoint.\n",
      "pyvideoai.train_multiprocess:  248 - INFO - ReduceLROnPlateau scheduler is selected. The `scheduler.step(val_loss)` function will be called at the end of epoch (after validation), but not every iteration.\n",
      "pyvideoai.train_multiprocess:  250 - INFO - For the ReduceLROnPlateau scheduler, initialise using training stats (summary.csv) instead of the checkpoint.\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 100/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9854 - loss: 0.5750 - batch_acc: 0.6250 - acc: 0.8487        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 95s - lr: 0.00000001 - loss: 0.5750 - acc: 0.8487                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3217 - val_acc: 0.7085        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3217 - val_acc: 0.7085\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0100.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0099.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 101/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5242 - loss: 0.5756 - batch_acc: 0.8750 - acc: 0.8517        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5756 - acc: 0.8517                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3199 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3199 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0101.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 102/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5711 - loss: 0.5836 - batch_acc: 0.8750 - acc: 0.8484        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5836 - acc: 0.8484                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3200 - val_acc: 0.7007        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3200 - val_acc: 0.7007\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0102.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0101.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 103/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.1962 - loss: 0.5750 - batch_acc: 1.0000 - acc: 0.8475        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5750 - acc: 0.8475                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3230 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3230 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0103.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0102.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 104/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4392 - loss: 0.5773 - batch_acc: 0.8750 - acc: 0.8428        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5773 - acc: 0.8428                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3203 - val_acc: 0.6980        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3203 - val_acc: 0.6980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3237 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3237 - val_acc: 0.7046 - val_vid_acc_top1: 0.7255 - val_vid_acc_top5: 0.9438\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0104.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0103.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 105/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.6633 - loss: 0.5894 - batch_acc: 0.8750 - acc: 0.8414        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5894 - acc: 0.8414                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3206 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3206 - val_acc: 0.7026\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0105.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0104.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 106/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.8325 - loss: 0.5838 - batch_acc: 0.7500 - acc: 0.8470        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5838 - acc: 0.8470                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3214 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3214 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0106.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0105.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 107/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.2797 - loss: 0.5615 - batch_acc: 0.8750 - acc: 0.8585        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5615 - acc: 0.8585                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3232 - val_acc: 0.7000        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3232 - val_acc: 0.7000\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0107.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0106.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 108/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5447 - loss: 0.5699 - batch_acc: 0.7500 - acc: 0.8543        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5699 - acc: 0.8543                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3209 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3209 - val_acc: 0.7046\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0108.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0107.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 109/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.1218 - loss: 0.5807 - batch_acc: 1.0000 - acc: 0.8484        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5807 - acc: 0.8484                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3219 - val_acc: 0.6967        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3219 - val_acc: 0.6967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3252 - val_acc: 0.7021        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3252 - val_acc: 0.7021 - val_vid_acc_top1: 0.7281 - val_vid_acc_top5: 0.9451\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0109.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0108.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 110/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4116 - loss: 0.5621 - batch_acc: 0.8750 - acc: 0.8582        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5621 - acc: 0.8582                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3207 - val_acc: 0.6987        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3207 - val_acc: 0.6987\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0110.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0109.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 111/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.0667 - loss: 0.5678 - batch_acc: 0.7500 - acc: 0.8501        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5678 - acc: 0.8501                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3206 - val_acc: 0.6987        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3206 - val_acc: 0.6987\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0111.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0110.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 112/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.6987 - loss: 0.5687 - batch_acc: 0.7500 - acc: 0.8512        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5687 - acc: 0.8512                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3219 - val_acc: 0.7007        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3219 - val_acc: 0.7007\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0112.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0111.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 113/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.0529 - loss: 0.5630 - batch_acc: 1.0000 - acc: 0.8585        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5630 - acc: 0.8585                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3205 - val_acc: 0.6967        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3205 - val_acc: 0.6967\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0113.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0112.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 114/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4138 - loss: 0.5625 - batch_acc: 1.0000 - acc: 0.8590        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5625 - acc: 0.8590                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3211 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3211 - val_acc: 0.7026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3243 - val_acc: 0.7086        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3243 - val_acc: 0.7086 - val_vid_acc_top1: 0.7294 - val_vid_acc_top5: 0.9405\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0114.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0113.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 115/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4948 - loss: 0.5559 - batch_acc: 0.8750 - acc: 0.8523        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5559 - acc: 0.8523                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3181 - val_acc: 0.7033        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3181 - val_acc: 0.7033\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0115.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0114.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 116/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.2748 - loss: 0.5470 - batch_acc: 1.0000 - acc: 0.8576        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5470 - acc: 0.8576                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3217 - val_acc: 0.7000        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3217 - val_acc: 0.7000\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0116.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0115.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 117/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.3717 - loss: 0.5605 - batch_acc: 0.8750 - acc: 0.8559        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5605 - acc: 0.8559                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3199 - val_acc: 0.7000        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3199 - val_acc: 0.7000\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0117.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0116.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 118/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.1968 - loss: 0.5419 - batch_acc: 0.6250 - acc: 0.8680        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5419 - acc: 0.8680                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3247 - val_acc: 0.6993        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3247 - val_acc: 0.6993\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0118.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0117.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 119/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.0759 - loss: 0.5543 - batch_acc: 0.6250 - acc: 0.8607        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5543 - acc: 0.8607                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3218 - val_acc: 0.7000        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3218 - val_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3241 - val_acc: 0.7082        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3241 - val_acc: 0.7082 - val_vid_acc_top1: 0.7307 - val_vid_acc_top5: 0.9405\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0119.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0118.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 120/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.1433 - loss: 0.5805 - batch_acc: 0.7500 - acc: 0.8489        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5805 - acc: 0.8489                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3208 - val_acc: 0.7007        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3208 - val_acc: 0.7007\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0120.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0119.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 121/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.7231 - loss: 0.5390 - batch_acc: 0.7500 - acc: 0.8627        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5390 - acc: 0.8627                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3201 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3201 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0121.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0120.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 122/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.4072 - loss: 0.5444 - batch_acc: 0.5000 - acc: 0.8615        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5444 - acc: 0.8615                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3180 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3180 - val_acc: 0.7026\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0122.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0121.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 123/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.6499 - loss: 0.5478 - batch_acc: 1.0000 - acc: 0.8618        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5478 - acc: 0.8618                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3250 - val_acc: 0.6987        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3250 - val_acc: 0.6987\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0123.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0122.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 124/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4978 - loss: 0.5502 - batch_acc: 0.8750 - acc: 0.8618        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5502 - acc: 0.8618                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3237 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3237 - val_acc: 0.7039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3271 - val_acc: 0.7050        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3271 - val_acc: 0.7050 - val_vid_acc_top1: 0.7275 - val_vid_acc_top5: 0.9438\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0124.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0123.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 125/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.7481 - loss: 0.5518 - batch_acc: 0.7500 - acc: 0.8638        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5518 - acc: 0.8638                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3190 - val_acc: 0.7013        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3190 - val_acc: 0.7013\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0125.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0124.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 126/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4226 - loss: 0.5277 - batch_acc: 0.8750 - acc: 0.8643        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5277 - acc: 0.8643                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3208 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3208 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0126.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0125.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 127/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5764 - loss: 0.5374 - batch_acc: 1.0000 - acc: 0.8607        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5374 - acc: 0.8607                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3185 - val_acc: 0.7052        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3185 - val_acc: 0.7052\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0127.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0126.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 128/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4152 - loss: 0.5345 - batch_acc: 1.0000 - acc: 0.8694        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5345 - acc: 0.8694                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3193 - val_acc: 0.7013        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3193 - val_acc: 0.7013\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0128.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0127.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 129/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.7311 - loss: 0.5269 - batch_acc: 0.7500 - acc: 0.8615        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5269 - acc: 0.8615                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3256 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3256 - val_acc: 0.7026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3284 - val_acc: 0.7034        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3284 - val_acc: 0.7034 - val_vid_acc_top1: 0.7268 - val_vid_acc_top5: 0.9346\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0129.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0128.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 130/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4393 - loss: 0.5375 - batch_acc: 0.8750 - acc: 0.8643        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5375 - acc: 0.8643                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3191 - val_acc: 0.7013        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3191 - val_acc: 0.7013\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0130.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0129.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 131/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5781 - loss: 0.5200 - batch_acc: 1.0000 - acc: 0.8680        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5200 - acc: 0.8680                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3203 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3203 - val_acc: 0.7046\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0131.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0130.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 132/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9742 - loss: 0.5269 - batch_acc: 0.7500 - acc: 0.8714        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5269 - acc: 0.8714                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3250 - val_acc: 0.6954        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3250 - val_acc: 0.6954\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0132.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0131.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 133/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4162 - loss: 0.5455 - batch_acc: 1.0000 - acc: 0.8660        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5455 - acc: 0.8660                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3208 - val_acc: 0.6993        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3208 - val_acc: 0.6993\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0133.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0132.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 134/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.2715 - loss: 0.5358 - batch_acc: 1.0000 - acc: 0.8688        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5358 - acc: 0.8688                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3183 - val_acc: 0.7085        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3183 - val_acc: 0.7085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3218 - val_acc: 0.7084        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3218 - val_acc: 0.7084 - val_vid_acc_top1: 0.7275 - val_vid_acc_top5: 0.9418\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0134.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0133.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 135/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.3408 - loss: 0.5464 - batch_acc: 1.0000 - acc: 0.8629        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5464 - acc: 0.8629                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3212 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3212 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0135.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 136/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.7651 - loss: 0.5401 - batch_acc: 0.7500 - acc: 0.8674        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5401 - acc: 0.8674                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3198 - val_acc: 0.7000        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3198 - val_acc: 0.7000\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0136.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0135.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 137/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5749 - loss: 0.5262 - batch_acc: 0.8750 - acc: 0.8722        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5262 - acc: 0.8722                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3246 - val_acc: 0.7072        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3246 - val_acc: 0.7072\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0137.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0136.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 138/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4100 - loss: 0.5355 - batch_acc: 0.8750 - acc: 0.8677        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5355 - acc: 0.8677                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3176 - val_acc: 0.7078        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3176 - val_acc: 0.7078\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0138.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0137.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 139/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.1583 - loss: 0.5274 - batch_acc: 0.8750 - acc: 0.8714        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5274 - acc: 0.8714                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3240 - val_acc: 0.6954        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3240 - val_acc: 0.6954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3264 - val_acc: 0.7018        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3264 - val_acc: 0.7018 - val_vid_acc_top1: 0.7294 - val_vid_acc_top5: 0.9386\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0139.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0138.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 140/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.6454 - loss: 0.5314 - batch_acc: 0.7500 - acc: 0.8686        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5314 - acc: 0.8686                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3279 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3279 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0140.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0139.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 141/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.7825 - loss: 0.5200 - batch_acc: 0.8750 - acc: 0.8756        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5200 - acc: 0.8756                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3185 - val_acc: 0.7013        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3185 - val_acc: 0.7013\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0141.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0140.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 142/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.3122 - loss: 0.5226 - batch_acc: 1.0000 - acc: 0.8747        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5226 - acc: 0.8747                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3180 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3180 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0142.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0141.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 143/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.4654 - loss: 0.5068 - batch_acc: 0.5000 - acc: 0.8803        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5068 - acc: 0.8803                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3226 - val_acc: 0.6993        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3226 - val_acc: 0.6993\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0143.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0142.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 144/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9427 - loss: 0.5213 - batch_acc: 0.7500 - acc: 0.8697        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5213 - acc: 0.8697                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3201 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3201 - val_acc: 0.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3230 - val_acc: 0.7043        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3230 - val_acc: 0.7043 - val_vid_acc_top1: 0.7314 - val_vid_acc_top5: 0.9412\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0144.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0143.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 145/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9231 - loss: 0.5109 - batch_acc: 0.7500 - acc: 0.8831        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5109 - acc: 0.8831                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3245 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3245 - val_acc: 0.7026\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0145.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0144.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 146/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.2430 - loss: 0.5109 - batch_acc: 1.0000 - acc: 0.8772        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5109 - acc: 0.8772                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3228 - val_acc: 0.6980        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3228 - val_acc: 0.6980\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0146.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0145.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 147/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.3461 - loss: 0.5183 - batch_acc: 0.6250 - acc: 0.8758        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5183 - acc: 0.8758                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3192 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3192 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0147.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0146.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 148/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5289 - loss: 0.5912 - batch_acc: 0.7500 - acc: 0.8447        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5912 - acc: 0.8447                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3219 - val_acc: 0.6980        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3219 - val_acc: 0.6980\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0148.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0147.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 149/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.3997 - loss: 0.5795 - batch_acc: 1.0000 - acc: 0.8506        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5795 - acc: 0.8506                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3184 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3184 - val_acc: 0.7046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3217 - val_acc: 0.7033        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3217 - val_acc: 0.7033 - val_vid_acc_top1: 0.7268 - val_vid_acc_top5: 0.9399\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0149.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0148.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 150/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5972 - loss: 0.5810 - batch_acc: 0.8750 - acc: 0.8489        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5810 - acc: 0.8489                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3197 - val_acc: 0.7065        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3197 - val_acc: 0.7065\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0150.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0149.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 151/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5305 - loss: 0.5757 - batch_acc: 1.0000 - acc: 0.8475        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5757 - acc: 0.8475                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3209 - val_acc: 0.6980        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3209 - val_acc: 0.6980\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0151.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0150.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 152/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.0583 - loss: 0.5956 - batch_acc: 1.0000 - acc: 0.8433        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5956 - acc: 0.8433                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3191 - val_acc: 0.7000        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3191 - val_acc: 0.7000\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0152.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0151.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 153/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.7539 - loss: 0.5915 - batch_acc: 0.8750 - acc: 0.8408        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5915 - acc: 0.8408                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3168 - val_acc: 0.7000        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3168 - val_acc: 0.7000\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0153.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0152.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 154/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.4638 - loss: 0.5926 - batch_acc: 0.6250 - acc: 0.8442        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5926 - acc: 0.8442                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3210 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3210 - val_acc: 0.7039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3246 - val_acc: 0.7054        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3246 - val_acc: 0.7054 - val_vid_acc_top1: 0.7248 - val_vid_acc_top5: 0.9425\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0154.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0153.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 155/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4890 - loss: 0.5924 - batch_acc: 0.8750 - acc: 0.8464        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5924 - acc: 0.8464                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3214 - val_acc: 0.7052        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3214 - val_acc: 0.7052\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0155.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0154.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 156/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.3303 - loss: 0.6109 - batch_acc: 0.7500 - acc: 0.8352        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6109 - acc: 0.8352                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3236 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3236 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0156.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0155.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 157/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5051 - loss: 0.6031 - batch_acc: 0.8750 - acc: 0.8442        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6031 - acc: 0.8442                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3208 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3208 - val_acc: 0.7026\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0157.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0156.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 158/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.7397 - loss: 0.5893 - batch_acc: 0.8750 - acc: 0.8478        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5893 - acc: 0.8478                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3211 - val_acc: 0.7013        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3211 - val_acc: 0.7013\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0158.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0157.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 159/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4075 - loss: 0.5856 - batch_acc: 0.8750 - acc: 0.8492        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5856 - acc: 0.8492                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3226 - val_acc: 0.7052        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3226 - val_acc: 0.7052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3244 - val_acc: 0.7061        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3244 - val_acc: 0.7061 - val_vid_acc_top1: 0.7301 - val_vid_acc_top5: 0.9425\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0159.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0158.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 160/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.7322 - loss: 0.5880 - batch_acc: 0.7500 - acc: 0.8473        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5880 - acc: 0.8473                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3206 - val_acc: 0.7078        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3206 - val_acc: 0.7078\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0160.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0159.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 161/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5106 - loss: 0.6072 - batch_acc: 1.0000 - acc: 0.8386        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6072 - acc: 0.8386                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3213 - val_acc: 0.6987        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3213 - val_acc: 0.6987\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0161.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0160.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 162/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.2849 - loss: 0.6005 - batch_acc: 1.0000 - acc: 0.8442        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6005 - acc: 0.8442                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3242 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3242 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0162.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0161.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 163/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9272 - loss: 0.5852 - batch_acc: 0.8750 - acc: 0.8419        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5852 - acc: 0.8419                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3309 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3309 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0163.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0162.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 164/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9380 - loss: 0.5967 - batch_acc: 0.6250 - acc: 0.8487        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5967 - acc: 0.8487                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3213 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3213 - val_acc: 0.7039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3248 - val_acc: 0.7085        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3248 - val_acc: 0.7085 - val_vid_acc_top1: 0.7307 - val_vid_acc_top5: 0.9386\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0164.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0163.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 165/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.3107 - loss: 0.5889 - batch_acc: 0.5000 - acc: 0.8489        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5889 - acc: 0.8489                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3237 - val_acc: 0.6980        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3237 - val_acc: 0.6980\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0165.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0164.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 166/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.1378 - loss: 0.5997 - batch_acc: 1.0000 - acc: 0.8366        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5997 - acc: 0.8366                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3203 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3203 - val_acc: 0.7026\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0166.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0165.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 167/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9830 - loss: 0.5944 - batch_acc: 0.6250 - acc: 0.8459        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5944 - acc: 0.8459                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3220 - val_acc: 0.6987        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3220 - val_acc: 0.6987\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0167.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0166.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 168/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.3071 - loss: 0.6118 - batch_acc: 0.8750 - acc: 0.8344        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6118 - acc: 0.8344                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3202 - val_acc: 0.7007        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3202 - val_acc: 0.7007\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0168.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0167.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 169/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.7961 - loss: 0.6069 - batch_acc: 0.7500 - acc: 0.8346        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6069 - acc: 0.8346                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3224 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3224 - val_acc: 0.7026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3250 - val_acc: 0.7069        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3250 - val_acc: 0.7069 - val_vid_acc_top1: 0.7307 - val_vid_acc_top5: 0.9405\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0169.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0168.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 170/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9098 - loss: 0.6077 - batch_acc: 0.7500 - acc: 0.8374        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6077 - acc: 0.8374                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3194 - val_acc: 0.7013        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3194 - val_acc: 0.7013\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0170.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0169.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 171/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.8107 - loss: 0.6043 - batch_acc: 0.8750 - acc: 0.8374        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6043 - acc: 0.8374                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3181 - val_acc: 0.7072        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3181 - val_acc: 0.7072\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0171.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0170.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 172/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.3912 - loss: 0.6008 - batch_acc: 0.6250 - acc: 0.8391        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6008 - acc: 0.8391                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3229 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3229 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0172.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0171.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 173/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.8926 - loss: 0.5936 - batch_acc: 0.7500 - acc: 0.8430        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5936 - acc: 0.8430                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3191 - val_acc: 0.7000        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3191 - val_acc: 0.7000\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0173.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0172.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 174/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.7303 - loss: 0.5988 - batch_acc: 0.7500 - acc: 0.8473        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5988 - acc: 0.8473                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3209 - val_acc: 0.7065        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3209 - val_acc: 0.7065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3242 - val_acc: 0.7071        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3242 - val_acc: 0.7071 - val_vid_acc_top1: 0.7242 - val_vid_acc_top5: 0.9392\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0174.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0173.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 175/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4875 - loss: 0.5827 - batch_acc: 1.0000 - acc: 0.8489        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5827 - acc: 0.8489                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3183 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3183 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0175.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0174.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 176/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5337 - loss: 0.5995 - batch_acc: 0.7500 - acc: 0.8391        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5995 - acc: 0.8391                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3197 - val_acc: 0.7059        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3197 - val_acc: 0.7059\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0176.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0175.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 177/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.8395 - loss: 0.6007 - batch_acc: 0.8750 - acc: 0.8436        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6007 - acc: 0.8436                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3183 - val_acc: 0.7052        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3183 - val_acc: 0.7052\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0177.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0176.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 178/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9177 - loss: 0.5856 - batch_acc: 0.7500 - acc: 0.8481        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5856 - acc: 0.8481                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3239 - val_acc: 0.7013        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3239 - val_acc: 0.7013\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0178.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0177.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 179/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.1287 - loss: 0.5965 - batch_acc: 0.6250 - acc: 0.8489        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5965 - acc: 0.8489                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3235 - val_acc: 0.7033        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3235 - val_acc: 0.7033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3262 - val_acc: 0.7033        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3262 - val_acc: 0.7033 - val_vid_acc_top1: 0.7261 - val_vid_acc_top5: 0.9399\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0179.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0178.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 180/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.0339 - loss: 0.5944 - batch_acc: 0.7500 - acc: 0.8459        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5944 - acc: 0.8459                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3204 - val_acc: 0.6980        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3204 - val_acc: 0.6980\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0180.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0179.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 181/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.0763 - loss: 0.5803 - batch_acc: 1.0000 - acc: 0.8473        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5803 - acc: 0.8473                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3211 - val_acc: 0.6993        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3211 - val_acc: 0.6993\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0181.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0180.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 182/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9557 - loss: 0.6174 - batch_acc: 0.7500 - acc: 0.8341        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6174 - acc: 0.8341                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3194 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3194 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0182.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0181.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 183/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4808 - loss: 0.5932 - batch_acc: 0.8750 - acc: 0.8456        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5932 - acc: 0.8456                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3225 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3225 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0183.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0182.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 184/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.6332 - loss: 0.6036 - batch_acc: 0.8750 - acc: 0.8411        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6036 - acc: 0.8411                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3184 - val_acc: 0.7078        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3184 - val_acc: 0.7078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3202 - val_acc: 0.7114        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3202 - val_acc: 0.7114 - val_vid_acc_top1: 0.7320 - val_vid_acc_top5: 0.9405\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0184.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0183.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 185/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.8374 - loss: 0.5943 - batch_acc: 0.8750 - acc: 0.8464        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5943 - acc: 0.8464                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3237 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3237 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0185.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0184.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 186/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.2304 - loss: 0.5844 - batch_acc: 1.0000 - acc: 0.8456        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5844 - acc: 0.8456                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3197 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3197 - val_acc: 0.7020\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0186.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0185.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 187/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.1432 - loss: 0.6019 - batch_acc: 0.7500 - acc: 0.8332        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6019 - acc: 0.8332                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3225 - val_acc: 0.7052        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3225 - val_acc: 0.7052\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0187.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0186.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 188/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5862 - loss: 0.5880 - batch_acc: 0.8750 - acc: 0.8489        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5880 - acc: 0.8489                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3205 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3205 - val_acc: 0.7026\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0188.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0187.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 189/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.1097 - loss: 0.5854 - batch_acc: 1.0000 - acc: 0.8400        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5854 - acc: 0.8400                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3200 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3200 - val_acc: 0.7039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3231 - val_acc: 0.7055        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3231 - val_acc: 0.7055 - val_vid_acc_top1: 0.7255 - val_vid_acc_top5: 0.9399\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0189.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0188.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 190/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.5100 - loss: 0.6096 - batch_acc: 0.8750 - acc: 0.8374        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6096 - acc: 0.8374                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3231 - val_acc: 0.6987        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3231 - val_acc: 0.6987\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0190.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0189.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 191/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.3482 - loss: 0.6026 - batch_acc: 0.8750 - acc: 0.8391        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6026 - acc: 0.8391                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3209 - val_acc: 0.7059        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3209 - val_acc: 0.7059\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0191.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0190.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 192/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.6989 - loss: 0.6063 - batch_acc: 0.8750 - acc: 0.8380        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6063 - acc: 0.8380                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3185 - val_acc: 0.7039        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3185 - val_acc: 0.7039\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0192.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0191.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 193/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.3695 - loss: 0.5948 - batch_acc: 1.0000 - acc: 0.8408        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5948 - acc: 0.8408                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3216 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3216 - val_acc: 0.7046\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0193.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0192.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 194/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.3701 - loss: 0.5908 - batch_acc: 0.8750 - acc: 0.8416        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5908 - acc: 0.8416                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3211 - val_acc: 0.7020        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3211 - val_acc: 0.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3253 - val_acc: 0.7043        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3253 - val_acc: 0.7043 - val_vid_acc_top1: 0.7294 - val_vid_acc_top5: 0.9425\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0194.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0193.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 195/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.4364 - loss: 0.5888 - batch_acc: 0.8750 - acc: 0.8475        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5888 - acc: 0.8475                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3194 - val_acc: 0.7026        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3194 - val_acc: 0.7026\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0195.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0194.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 196/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.0889 - loss: 0.5886 - batch_acc: 1.0000 - acc: 0.8439        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5886 - acc: 0.8439                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3194 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3194 - val_acc: 0.7046\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0196.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0195.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 197/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.2391 - loss: 0.6019 - batch_acc: 0.6250 - acc: 0.8456        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.6019 - acc: 0.8456                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3184 - val_acc: 0.7033        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3184 - val_acc: 0.7033\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0197.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0196.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 198/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.1727 - loss: 0.5969 - batch_acc: 0.6250 - acc: 0.8461        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5969 - acc: 0.8461                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3158 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3158 - val_acc: 0.7046\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0198.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0197.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 199/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.8650 - loss: 0.5864 - batch_acc: 0.7500 - acc: 0.8422        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5864 - acc: 0.8422                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3207 - val_acc: 0.7033        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3207 - val_acc: 0.7033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3233 - val_acc: 0.7082        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3233 - val_acc: 0.7082 - val_vid_acc_top1: 0.7294 - val_vid_acc_top5: 0.9405\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0199.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0198.pth\n",
      "pyvideoai.train_multiprocess:  455 - SUCCESS - Finished training\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%run \"$PYVIDEOAI_DIR/tools/run_train.py\" --local_world_size 1 -D hmdb -M i3d_resnet50 -N crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1 -e 200 -l -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c952349-711a-43bd-bc4c-b209e876fd4b",
   "metadata": {},
   "source": [
    "### From the intermediate checkpoint\n",
    "Go to the experiment folder, and find the summary file in `data/experiments/hmdb/i3d_resnet/crop224_lr001_batch8_8x8_largejit_steplr_1scrop5tcrop_split1/logs/summary.csv`.  \n",
    "You'll see the training stats. Just remove the lines after your resuming point.  \n",
    "For example, you want to resume from the 50th epoch and start from 51st epoch, then remove from epoch 51 till the end.  \n",
    "Add `-l 50` to load the 50th epoch's checkpoint and resume from the 51st epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f746e-cb29-4c2a-8266-8296ab0de534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: Before running the code, make sure you change the summary.csv file!!\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%run \"$PYVIDEOAI_DIR/tools/run_train.py\" --local_world_size 1 -D hmdb -M i3d_resnet50 -N crop224_lr0001_batch8_8x8_largejit_steplr_1scrop5tcrop_split1 -e 200 -l 50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
