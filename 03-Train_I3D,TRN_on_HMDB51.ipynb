{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44eb80d-bdd5-4f1f-954f-c8c10301d679",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now the fun part begins. **Let's train the I3D network on the HMDB-51 dataset!**  \n",
    "\n",
    "First, we want to download the pre-trained weights from the [MODEL_ZOO.md](https://github.com/kiyoon/PyVideoAI/blob/master/MODEL_ZOO.md)  \n",
    "We'll use the I3D pretrained on the Kinetics-400 dataset, with 8-frame input.  \n",
    "\n",
    "Note that the path to the pretrained weights is defined in `model_configs/i3d_resnet50.py` as below.  \n",
    "```python\n",
    "kinetics400_pretrained_path_8x8 = os.path.join(DATA_DIR, 'pretrained', 'kinetics400/i3d_resnet50/I3D_8x8_R50.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b00c0c1-2cb7-41fa-aff9-916f779a8cda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYVIDEOAI_DIR=/home/kiyoon/project/PyVideoAI\n",
      "env: DATA_DIR=/home/kiyoon/project/PyVideoAI/data\n",
      "env: HDD_PATH=/mnt/hdd/kiyoon\n"
     ]
    }
   ],
   "source": [
    "## IMPORTANT: You must change path values in `00-storage_location.py` before executing below.\n",
    "# Environments for future use\n",
    "\n",
    "from pyvideoai.config import PYVIDEOAI_DIR, DATA_DIR\n",
    "%env PYVIDEOAI_DIR=$PYVIDEOAI_DIR\n",
    "%env DATA_DIR=$DATA_DIR\n",
    "\n",
    "import os\n",
    "exec(open(os.path.join(PYVIDEOAI_DIR, 'examples', '00-storage_location.py')).read())\n",
    "%env HDD_PATH=$HDD_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6efdac-059c-46d4-a419-8856d6b8c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/home/kiyoon/project/PyVideoAI/data/pretrained': File exists\n",
      "--2022-08-19 17:22:40--  https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/I3D_8x8_R50.pkl\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 2606:4700:10::ac43:904, 2606:4700:10::6816:4a8e, 2606:4700:10::6816:4b8e, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|2606:4700:10::ac43:904|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 224598662 (214M) [application/octet-stream]\n",
      "Saving to: ‘/home/kiyoon/project/PyVideoAI/data/pretrained/kinetics400/i3d_resnet50/I3D_8x8_R50.pkl’\n",
      "\n",
      "I3D_8x8_R50.pkl     100%[===================>] 214.19M  20.8MB/s    in 11s     \n",
      "\n",
      "2022-08-19 17:22:52 (19.1 MB/s) - ‘/home/kiyoon/project/PyVideoAI/data/pretrained/kinetics400/i3d_resnet50/I3D_8x8_R50.pkl’ saved [224598662/224598662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Link the pretrained weight directory to HDD\n",
    "!mkdir -p \"$HDD_PATH/pretrained/kinetics400/i3d_resnet50\"\n",
    "!ln -s \"$HDD_PATH/pretrained\" \"$DATA_DIR/\"\n",
    "\n",
    "# Download\n",
    "!wget https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/I3D_8x8_R50.pkl -P \"$DATA_DIR/pretrained/kinetics400/i3d_resnet50\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c762f-6e2b-4841-91c3-82a299e00600",
   "metadata": {},
   "source": [
    "Now we have our datasets processed, and the pretrained weights ready.  \n",
    "We want to use three config files.  \n",
    "- `hmdb.py` in `dataset_configs`,  \n",
    "- `i3d_resnet50.py` in `model_configs`,  \n",
    "- `hmdb/i3d_resnet50-crop224_8x8_largejit_plateau_1scrop5tcrop_split1.py` in `exp_configs`.\n",
    "\n",
    "NOTE: The only difference between the inference example is the `load_pretrained(model)` function definition in the exp_config file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790f1cd2-a928-45fa-8bc6-3b9058d2f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Setup Telegram bot to report the experiment stats\n",
    "import os\n",
    "if not os.path.isfile(f\"{PYVIDEOAI_DIR}/tools/key.ini\"):\n",
    "    !cp \"$PYVIDEOAI_DIR/tools/key.ini\"{.template,}\n",
    "# EDIT the `tools/key.ini` file on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823702ea-6418-4108-9c42-86d8126fe17f",
   "metadata": {},
   "source": [
    "### Optional: Set up Weights & Biases\n",
    "\n",
    "You need to set up W & B\n",
    "```bash\n",
    "pip install wandb\n",
    "wandb login\n",
    "```\n",
    "Just add `--wandb_project test` argument below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02bf4ff2-68f2-4f49-a76a-eef25f45ef8a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  122 - \u001b[1;30mINFO\u001b[0m - PyTorch==1.10.1\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  123 - \u001b[1;30mINFO\u001b[0m - PyVideoAI==v0.3+236.g4034acb.dirty\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  124 - \u001b[1;30mINFO\u001b[0m - Experiment folder: /mnt/hdd/kiyoon/PyVideoAI_experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_000 on host rossum\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  181 - \u001b[1;30mINFO\u001b[0m - args: {\n",
      "    \"num_epochs\": \"DEPRECATED\",\n",
      "    \"experiment_root\": \"/mnt/hdd/kiyoon/PyVideoAI_experiments\",\n",
      "    \"dataset\": \"hmdb\",\n",
      "    \"model\": \"i3d_resnet50\",\n",
      "    \"experiment_name\": \"crop224_8x8_largejit_plateau_1scrop5tcrop_split1\",\n",
      "    \"subfolder_name\": null,\n",
      "    \"dataset_channel\": null,\n",
      "    \"model_channel\": null,\n",
      "    \"experiment_channel\": null,\n",
      "    \"save_mode\": \"last_and_peaks\",\n",
      "    \"training_speed\": \"standard\",\n",
      "    \"load_epoch\": null,\n",
      "    \"seed\": 12,\n",
      "    \"multi_crop_val_period\": -1,\n",
      "    \"telegram_post_period\": 10,\n",
      "    \"telegram_bot_idx\": 0,\n",
      "    \"wandb_project\": null,\n",
      "    \"wandb_entity\": null,\n",
      "    \"wandb_run_id\": null,\n",
      "    \"wandb_upload_models\": \"best_and_last\",\n",
      "    \"dataloader_num_workers\": 4,\n",
      "    \"refresh_period\": 1,\n",
      "    \"version\": \"auto\",\n",
      "    \"console_log_level\": \"INFO\"\n",
      "}\n",
      "\u001b[34mpyvideoai.utils.distributed:\u001b[0m  395 - \u001b[1;30mINFO\u001b[0m - distributed configs: {\n",
      "    \"world_size\": 1,\n",
      "    \"local_world_size\": 1,\n",
      "    \"num_nodes (estimated)\": 1\n",
      "}\n",
      "\u001b[34mpyvideoai.dataloaders.frames_densesample_dataset:\u001b[0m  150 - \u001b[1;30mINFO\u001b[0m - Constructing video dataset train...\n",
      "\u001b[34mpyvideoai.dataloaders.frames_densesample_dataset:\u001b[0m  223 - \u001b[1;30mINFO\u001b[0m - Constructing video dataloader (size: 3570) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/train1.csv\n",
      "\u001b[34mpyvideoai.dataloaders.frames_densesample_dataset:\u001b[0m  150 - \u001b[1;30mINFO\u001b[0m - Constructing video dataset test...\n",
      "\u001b[34mpyvideoai.dataloaders.frames_densesample_dataset:\u001b[0m  223 - \u001b[1;30mINFO\u001b[0m - Constructing video dataloader (size: 1530) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/test1.csv\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  212 - \u001b[1;30mINFO\u001b[0m - Using batch size of 16 per process (per GPU), resulting in total size of 16.\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  213 - \u001b[1;30mINFO\u001b[0m - Using validation batch size of 16 per process (per GPU), resulting in total size of 16.\n",
      "\u001b[34mpyvideoai.utils.misc:\u001b[0m   52 - \u001b[1;30mINFO\u001b[0m - Model:\n",
      "ResNetModel(\n",
      "  (s1): VideoModelStem(\n",
      "    (pathway0_stem): ResNetBasicStem(\n",
      "      (conv): Conv3d(3, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)\n",
      "      (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (pool_layer): MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (s2): ResStage(\n",
      "    (pathway0_res0): ResBlock(\n",
      "      (branch1): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (branch1_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (a_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res1): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(256, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (a_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res2): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(256, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (a_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pathway0_pool): MaxPool3d(kernel_size=[2, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=1, ceil_mode=False)\n",
      "  (s3): ResStage(\n",
      "    (pathway0_res0): ResBlock(\n",
      "      (branch1): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "      (branch1_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(256, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res1): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res2): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(512, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res3): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (s4): ResStage(\n",
      "    (pathway0_res0): ResBlock(\n",
      "      (branch1): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "      (branch1_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(512, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res1): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res2): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res3): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res4): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res5): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (s5): ResStage(\n",
      "    (pathway0_res0): ResBlock(\n",
      "      (branch1): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "      (branch1_bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (a_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res1): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(2048, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "        (a_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (pathway0_res2): ResBlock(\n",
      "      (branch2): BottleneckTransform(\n",
      "        (a): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (a_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (a_relu): ReLU(inplace=True)\n",
      "        (b): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "        (b_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (b_relu): ReLU(inplace=True)\n",
      "        (c): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (c_bn): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (head): ResNetBasicHead(\n",
      "    (pathway0_avgpool): AvgPool3d(kernel_size=[4, 7, 7], stride=1, padding=0)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (projection): Linear(in_features=2048, out_features=51, bias=True)\n",
      "    (act): Softmax(dim=4)\n",
      "  )\n",
      ")\n",
      "\u001b[34mpyvideoai.utils.misc:\u001b[0m   53 - \u001b[1;30mINFO\u001b[0m - Params: 27,328,371\n",
      "\u001b[34mpyvideoai.utils.misc:\u001b[0m   54 - \u001b[1;30mINFO\u001b[0m - Mem: 104.4853515625 MiB\n",
      "\u001b[34mpyvideoai.utils.misc:\u001b[0m   55 - \u001b[1;30mINFO\u001b[0m - nvidia-smi\n",
      "\u001b[34mpyvideoai.utils.misc:\u001b[0m   56 - \u001b[1;30mINFO\u001b[0m - Fri Aug 19 17:24:56 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:19:00.0 Off |                  N/A |\n",
      "|  0%   33C    P2    66W / 250W |   1112MiB / 11019MiB |      5%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:1A:00.0 Off |                  N/A |\n",
      "|  0%   32C    P8    41W / 250W |      1MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:67:00.0 Off |                  N/A |\n",
      "|  0%   35C    P8    23W / 250W |      1MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:68:00.0 Off |                  N/A |\n",
      "|  0%   35C    P8    23W / 250W |      1MiB / 11016MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1724      C   python                           1109MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[34mpyvideoai.utils.misc:\u001b[0m  192 - \u001b[1;30mINFO\u001b[0m - Pillow-SIMD running with version 9.0.0.post1\n",
      "\u001b[34mpyvideoai.utils.misc:\u001b[0m  203 - \u001b[1;30mINFO\u001b[0m - libjpeg-turbo is on. Jpeg encoding/decoding with Pillow will be faster.\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res_conv1_bn_b: (64,) => s1.pathway0_stem.bn.bias: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res_conv1_bn_s: (64,) => s1.pathway0_stem.bn.weight: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2a_bn_rm: (512,) => s5.pathway0_res2.branch2.a_bn.running_mean: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res4.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2c_bn_rm: (2048,) => s5.pathway0_res1.branch2.c_bn.running_mean: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2c_bn_s: (512,) => s3.pathway0_res1.branch2.c_bn.weight: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2b_w: (512, 512, 1, 3, 3) => s5.pathway0_res1.branch2.b.weight: (512, 512, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2a_bn_riv: (128,) => s3.pathway0_res0.branch2.a_bn.running_var: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2b_bn_rm: (64,) => s2.pathway0_res2.branch2.b_bn.running_mean: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2c_bn_riv: (2048,) => s5.pathway0_res0.branch2.c_bn.running_var: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2a_w: (128, 512, 3, 1, 1) => s3.pathway0_res2.branch2.a.weight: (128, 512, 3, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2c_bn_rm: (1024,) => s4.pathway0_res5.branch2.c_bn.running_mean: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2c_w: (512, 128, 1, 1, 1) => s3.pathway0_res2.branch2.c.weight: (512, 128, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2c_bn_rm: (1024,) => s4.pathway0_res0.branch2.c_bn.running_mean: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2a_bn_riv: (64,) => s2.pathway0_res2.branch2.a_bn.running_var: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2a_w: (256, 1024, 1, 1, 1) => s4.pathway0_res5.branch2.a.weight: (256, 1024, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2a_bn_b: (64,) => s2.pathway0_res2.branch2.a_bn.bias: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2a_bn_s: (64,) => s2.pathway0_res2.branch2.a_bn.weight: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2a_bn_rm: (128,) => s3.pathway0_res2.branch2.a_bn.running_mean: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res0.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2a_bn_b: (128,) => s3.pathway0_res2.branch2.a_bn.bias: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2b_bn_riv: (256,) => s4.pathway0_res1.branch2.b_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2b_bn_b: (256,) => s4.pathway0_res4.branch2.b_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2c_bn_b: (512,) => s3.pathway0_res1.branch2.c_bn.bias: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2c_bn_riv: (1024,) => s4.pathway0_res0.branch2.c_bn.running_var: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2b_bn_riv: (128,) => s3.pathway0_res3.branch2.b_bn.running_var: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2a_bn_riv: (512,) => s5.pathway0_res1.branch2.a_bn.running_var: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2b_bn_s: (256,) => s4.pathway0_res4.branch2.b_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch1_bn_rm: (2048,) => s5.pathway0_res0.branch1_bn.running_mean: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2a_bn_rm: (64,) => s2.pathway0_res0.branch2.a_bn.running_mean: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2a_bn_rm: (256,) => s4.pathway0_res4.branch2.a_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2a_bn_rm: (256,) => s4.pathway0_res0.branch2.a_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch1_w: (512, 256, 1, 1, 1) => s3.pathway0_res0.branch1.weight: (512, 256, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2c_bn_riv: (1024,) => s4.pathway0_res5.branch2.c_bn.running_var: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2a_bn_rm: (64,) => s2.pathway0_res1.branch2.a_bn.running_mean: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2a_bn_rm: (128,) => s3.pathway0_res3.branch2.a_bn.running_mean: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2a_w: (256, 1024, 1, 1, 1) => s4.pathway0_res1.branch2.a.weight: (256, 1024, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch1_bn_rm: (512,) => s3.pathway0_res0.branch1_bn.running_mean: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res1.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res5.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2c_bn_b: (512,) => s3.pathway0_res3.branch2.c_bn.bias: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2c_bn_s: (512,) => s3.pathway0_res3.branch2.c_bn.weight: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2a_w: (512, 2048, 3, 1, 1) => s5.pathway0_res1.branch2.a.weight: (512, 2048, 3, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2a_bn_riv: (512,) => s5.pathway0_res0.branch2.a_bn.running_var: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2c_bn_riv: (256,) => s2.pathway0_res1.branch2.c_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2b_bn_b: (256,) => s4.pathway0_res5.branch2.b_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2b_bn_s: (256,) => s4.pathway0_res5.branch2.b_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2c_bn_b: (1024,) => s4.pathway0_res4.branch2.c_bn.bias: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2c_w: (2048, 512, 1, 1, 1) => s5.pathway0_res1.branch2.c.weight: (2048, 512, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2a_w: (256, 1024, 3, 1, 1) => s4.pathway0_res4.branch2.a.weight: (256, 1024, 3, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2b_bn_rm: (256,) => s4.pathway0_res4.branch2.b_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2b_bn_rm: (512,) => s5.pathway0_res1.branch2.b_bn.running_mean: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2b_bn_b: (512,) => s5.pathway0_res1.branch2.b_bn.bias: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2a_bn_riv: (128,) => s3.pathway0_res1.branch2.a_bn.running_var: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2b_bn_s: (512,) => s5.pathway0_res1.branch2.b_bn.weight: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res4.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2c_bn_riv: (256,) => s2.pathway0_res2.branch2.c_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2c_bn_s: (1024,) => s4.pathway0_res2.branch2.c_bn.weight: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2c_bn_b: (1024,) => s4.pathway0_res2.branch2.c_bn.bias: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2a_w: (64, 256, 3, 1, 1) => s2.pathway0_res2.branch2.a.weight: (64, 256, 3, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2b_bn_riv: (64,) => s2.pathway0_res0.branch2.b_bn.running_var: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2b_bn_riv: (512,) => s5.pathway0_res2.branch2.b_bn.running_var: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch1_bn_riv: (2048,) => s5.pathway0_res0.branch1_bn.running_var: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2a_bn_rm: (64,) => s2.pathway0_res2.branch2.a_bn.running_mean: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2a_bn_s: (256,) => s4.pathway0_res1.branch2.a_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2c_bn_b: (256,) => s2.pathway0_res1.branch2.c_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2b_bn_b: (128,) => s3.pathway0_res2.branch2.b_bn.bias: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2b_bn_s: (128,) => s3.pathway0_res1.branch2.b_bn.weight: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2a_bn_b: (256,) => s4.pathway0_res1.branch2.a_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2c_bn_s: (1024,) => s4.pathway0_res4.branch2.c_bn.weight: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2c_bn_s: (256,) => s2.pathway0_res1.branch2.c_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2b_bn_s: (128,) => s3.pathway0_res2.branch2.b_bn.weight: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2c_bn_s: (2048,) => s5.pathway0_res1.branch2.c_bn.weight: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2c_bn_b: (2048,) => s5.pathway0_res1.branch2.c_bn.bias: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2b_bn_rm: (64,) => s2.pathway0_res1.branch2.b_bn.running_mean: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2b_bn_b: (256,) => s4.pathway0_res1.branch2.b_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2a_w: (64, 64, 3, 1, 1) => s2.pathway0_res0.branch2.a.weight: (64, 64, 3, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2b_bn_s: (256,) => s4.pathway0_res1.branch2.b_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2b_w: (512, 512, 1, 3, 3) => s5.pathway0_res0.branch2.b.weight: (512, 512, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2c_bn_rm: (512,) => s3.pathway0_res1.branch2.c_bn.running_mean: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2a_bn_s: (256,) => s4.pathway0_res4.branch2.a_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2b_bn_s: (256,) => s4.pathway0_res2.branch2.b_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2a_bn_b: (256,) => s4.pathway0_res4.branch2.a_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2b_bn_b: (256,) => s4.pathway0_res2.branch2.b_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2c_bn_rm: (1024,) => s4.pathway0_res3.branch2.c_bn.running_mean: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2c_bn_riv: (1024,) => s4.pathway0_res3.branch2.c_bn.running_var: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2c_bn_rm: (1024,) => s4.pathway0_res2.branch2.c_bn.running_mean: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2a_bn_s: (128,) => s3.pathway0_res0.branch2.a_bn.weight: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2a_bn_b: (128,) => s3.pathway0_res0.branch2.a_bn.bias: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2a_bn_riv: (128,) => s3.pathway0_res3.branch2.a_bn.running_var: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2a_bn_rm: (256,) => s4.pathway0_res5.branch2.a_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2b_bn_riv: (512,) => s5.pathway0_res0.branch2.b_bn.running_var: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2a_bn_b: (128,) => s3.pathway0_res1.branch2.a_bn.bias: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2a_bn_s: (128,) => s3.pathway0_res1.branch2.a_bn.weight: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2a_bn_s: (512,) => s5.pathway0_res1.branch2.a_bn.weight: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2a_bn_riv: (256,) => s4.pathway0_res0.branch2.a_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2a_bn_riv: (512,) => s5.pathway0_res2.branch2.a_bn.running_var: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2a_bn_b: (512,) => s5.pathway0_res1.branch2.a_bn.bias: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2b_bn_riv: (256,) => s4.pathway0_res2.branch2.b_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2b_bn_riv: (256,) => s4.pathway0_res5.branch2.b_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2b_w: (64, 64, 1, 3, 3) => s2.pathway0_res2.branch2.b.weight: (64, 64, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2a_bn_b: (128,) => s3.pathway0_res3.branch2.a_bn.bias: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2a_bn_riv: (64,) => s2.pathway0_res1.branch2.a_bn.running_var: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2a_bn_riv: (256,) => s4.pathway0_res2.branch2.a_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2a_bn_s: (128,) => s3.pathway0_res3.branch2.a_bn.weight: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2b_w: (128, 128, 1, 3, 3) => s3.pathway0_res0.branch2.b.weight: (128, 128, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2a_bn_riv: (256,) => s4.pathway0_res5.branch2.a_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2b_bn_riv: (256,) => s4.pathway0_res0.branch2.b_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2c_w: (2048, 512, 1, 1, 1) => s5.pathway0_res0.branch2.c.weight: (2048, 512, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2a_w: (512, 2048, 1, 1, 1) => s5.pathway0_res2.branch2.a.weight: (512, 2048, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2c_bn_rm: (256,) => s2.pathway0_res0.branch2.c_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch1_w: (2048, 1024, 1, 1, 1) => s5.pathway0_res0.branch1.weight: (2048, 1024, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2c_bn_b: (256,) => s2.pathway0_res2.branch2.c_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2c_bn_b: (1024,) => s4.pathway0_res5.branch2.c_bn.bias: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2b_bn_s: (256,) => s4.pathway0_res3.branch2.b_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2c_bn_riv: (512,) => s3.pathway0_res2.branch2.c_bn.running_var: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2a_w: (512, 1024, 1, 1, 1) => s5.pathway0_res0.branch2.a.weight: (512, 1024, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2c_bn_s: (1024,) => s4.pathway0_res5.branch2.c_bn.weight: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2c_bn_rm: (512,) => s3.pathway0_res3.branch2.c_bn.running_mean: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2b_bn_b: (256,) => s4.pathway0_res3.branch2.b_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2b_bn_rm: (64,) => s2.pathway0_res0.branch2.b_bn.running_mean: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2a_bn_rm: (512,) => s5.pathway0_res1.branch2.a_bn.running_mean: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2b_w: (512, 512, 1, 3, 3) => s5.pathway0_res2.branch2.b.weight: (512, 512, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2c_bn_riv: (1024,) => s4.pathway0_res4.branch2.c_bn.running_var: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2c_bn_b: (1024,) => s4.pathway0_res0.branch2.c_bn.bias: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2b_bn_s: (128,) => s3.pathway0_res3.branch2.b_bn.weight: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2a_w: (256, 1024, 3, 1, 1) => s4.pathway0_res2.branch2.a.weight: (256, 1024, 3, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2c_bn_s: (1024,) => s4.pathway0_res0.branch2.c_bn.weight: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2b_bn_rm: (256,) => s4.pathway0_res3.branch2.b_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2b_bn_b: (128,) => s3.pathway0_res3.branch2.b_bn.bias: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2c_bn_riv: (2048,) => s5.pathway0_res1.branch2.c_bn.running_var: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2a_bn_riv: (256,) => s4.pathway0_res1.branch2.a_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - conv1_w: (64, 3, 5, 7, 7) => s1.pathway0_stem.conv.weight: (64, 3, 5, 7, 7)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch1_bn_riv: (256,) => s2.pathway0_res0.branch1_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2c_bn_rm: (512,) => s3.pathway0_res2.branch2.c_bn.running_mean: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2c_bn_riv: (2048,) => s5.pathway0_res2.branch2.c_bn.running_var: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2c_bn_b: (1024,) => s4.pathway0_res1.branch2.c_bn.bias: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2b_bn_rm: (512,) => s5.pathway0_res0.branch2.b_bn.running_mean: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2b_bn_riv: (256,) => s4.pathway0_res4.branch2.b_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2c_bn_s: (1024,) => s4.pathway0_res1.branch2.c_bn.weight: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2c_bn_riv: (1024,) => s4.pathway0_res1.branch2.c_bn.running_var: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2b_bn_rm: (128,) => s3.pathway0_res1.branch2.b_bn.running_mean: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2c_w: (256, 64, 1, 1, 1) => s2.pathway0_res1.branch2.c.weight: (256, 64, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2a_bn_riv: (64,) => s2.pathway0_res0.branch2.a_bn.running_var: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2a_bn_rm: (512,) => s5.pathway0_res0.branch2.a_bn.running_mean: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2a_bn_b: (512,) => s5.pathway0_res0.branch2.a_bn.bias: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2b_bn_b: (128,) => s3.pathway0_res1.branch2.b_bn.bias: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2a_bn_s: (128,) => s3.pathway0_res2.branch2.a_bn.weight: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2a_bn_s: (512,) => s5.pathway0_res0.branch2.a_bn.weight: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2b_bn_s: (256,) => s4.pathway0_res0.branch2.b_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2a_bn_b: (256,) => s4.pathway0_res2.branch2.a_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2b_bn_s: (512,) => s5.pathway0_res2.branch2.b_bn.weight: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2c_bn_s: (1024,) => s4.pathway0_res3.branch2.c_bn.weight: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2b_bn_s: (512,) => s5.pathway0_res0.branch2.b_bn.weight: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2a_bn_s: (256,) => s4.pathway0_res2.branch2.a_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2b_bn_b: (512,) => s5.pathway0_res2.branch2.b_bn.bias: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2c_bn_b: (1024,) => s4.pathway0_res3.branch2.c_bn.bias: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2b_bn_rm: (128,) => s3.pathway0_res0.branch2.b_bn.running_mean: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2b_bn_b: (512,) => s5.pathway0_res0.branch2.b_bn.bias: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2a_bn_b: (512,) => s5.pathway0_res2.branch2.a_bn.bias: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2c_bn_riv: (512,) => s3.pathway0_res3.branch2.c_bn.running_var: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch1_bn_s: (256,) => s2.pathway0_res0.branch1_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2a_bn_s: (512,) => s5.pathway0_res2.branch2.a_bn.weight: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2c_bn_riv: (256,) => s2.pathway0_res0.branch2.c_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch1_bn_b: (256,) => s2.pathway0_res0.branch1_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2b_bn_rm: (256,) => s4.pathway0_res0.branch2.b_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch1_bn_s: (1024,) => s4.pathway0_res0.branch1_bn.weight: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2a_bn_rm: (256,) => s4.pathway0_res2.branch2.a_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2b_w: (64, 64, 1, 3, 3) => s2.pathway0_res0.branch2.b.weight: (64, 64, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch1_bn_b: (1024,) => s4.pathway0_res0.branch1_bn.bias: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2c_bn_rm: (1024,) => s4.pathway0_res1.branch2.c_bn.running_mean: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2a_bn_rm: (256,) => s4.pathway0_res3.branch2.a_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res2.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch1_bn_b: (512,) => s3.pathway0_res0.branch1_bn.bias: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch1_bn_s: (512,) => s3.pathway0_res0.branch1_bn.weight: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2b_bn_rm: (256,) => s4.pathway0_res1.branch2.b_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2b_bn_b: (256,) => s4.pathway0_res0.branch2.b_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2c_bn_riv: (512,) => s3.pathway0_res1.branch2.c_bn.running_var: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2c_bn_b: (512,) => s3.pathway0_res0.branch2.c_bn.bias: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2a_bn_rm: (128,) => s3.pathway0_res0.branch2.a_bn.running_mean: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2c_bn_s: (2048,) => s5.pathway0_res0.branch2.c_bn.weight: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2b_bn_rm: (256,) => s4.pathway0_res2.branch2.b_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2c_w: (2048, 512, 1, 1, 1) => s5.pathway0_res2.branch2.c.weight: (2048, 512, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2c_bn_s: (512,) => s3.pathway0_res0.branch2.c_bn.weight: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2c_bn_b: (2048,) => s5.pathway0_res0.branch2.c_bn.bias: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2b_bn_riv: (64,) => s2.pathway0_res2.branch2.b_bn.running_var: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2a_bn_riv: (256,) => s4.pathway0_res3.branch2.a_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2c_w: (512, 128, 1, 1, 1) => s3.pathway0_res0.branch2.c.weight: (512, 128, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2c_w: (256, 64, 1, 1, 1) => s2.pathway0_res2.branch2.c.weight: (256, 64, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2b_bn_riv: (128,) => s3.pathway0_res0.branch2.b_bn.running_var: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2a_w: (128, 256, 3, 1, 1) => s3.pathway0_res0.branch2.a.weight: (128, 256, 3, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2a_bn_s: (64,) => s2.pathway0_res0.branch2.a_bn.weight: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2a_bn_b: (256,) => s4.pathway0_res3.branch2.a_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2a_bn_s: (64,) => s2.pathway0_res1.branch2.a_bn.weight: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2c_bn_rm: (512,) => s3.pathway0_res0.branch2.c_bn.running_mean: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2c_bn_b: (512,) => s3.pathway0_res2.branch2.c_bn.bias: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2a_bn_b: (64,) => s2.pathway0_res0.branch2.a_bn.bias: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2b_bn_rm: (256,) => s4.pathway0_res5.branch2.b_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2a_bn_b: (64,) => s2.pathway0_res1.branch2.a_bn.bias: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res2.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2b_w: (128, 128, 1, 3, 3) => s3.pathway0_res1.branch2.b.weight: (128, 128, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2c_bn_s: (512,) => s3.pathway0_res2.branch2.c_bn.weight: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2a_bn_rm: (128,) => s3.pathway0_res1.branch2.a_bn.running_mean: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2c_w: (512, 128, 1, 1, 1) => s3.pathway0_res3.branch2.c.weight: (512, 128, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2b_bn_b: (64,) => s2.pathway0_res0.branch2.b_bn.bias: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2a_bn_s: (256,) => s4.pathway0_res0.branch2.a_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2a_w: (128, 512, 1, 1, 1) => s3.pathway0_res3.branch2.a.weight: (128, 512, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2b_bn_riv: (128,) => s3.pathway0_res1.branch2.b_bn.running_var: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2b_bn_s: (64,) => s2.pathway0_res0.branch2.b_bn.weight: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2a_bn_b: (256,) => s4.pathway0_res0.branch2.a_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2b_bn_rm: (512,) => s5.pathway0_res2.branch2.b_bn.running_mean: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2c_bn_b: (256,) => s2.pathway0_res0.branch2.c_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2b_bn_rm: (128,) => s3.pathway0_res3.branch2.b_bn.running_mean: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2c_bn_s: (256,) => s2.pathway0_res0.branch2.c_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res3.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2b_bn_b: (128,) => s3.pathway0_res0.branch2.b_bn.bias: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch1_w: (256, 64, 1, 1, 1) => s2.pathway0_res0.branch1.weight: (256, 64, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2c_bn_rm: (256,) => s2.pathway0_res1.branch2.c_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2b_bn_s: (128,) => s3.pathway0_res0.branch2.b_bn.weight: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2c_bn_rm: (2048,) => s5.pathway0_res2.branch2.c_bn.running_mean: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res_conv1_bn_riv: (64,) => s1.pathway0_stem.bn.running_var: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2a_bn_riv: (256,) => s4.pathway0_res4.branch2.a_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_1_branch2b_bn_riv: (512,) => s5.pathway0_res1.branch2.b_bn.running_var: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2c_bn_s: (2048,) => s5.pathway0_res2.branch2.c_bn.weight: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch2c_w: (256, 64, 1, 1, 1) => s2.pathway0_res0.branch2.c.weight: (256, 64, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_2_branch2c_bn_b: (2048,) => s5.pathway0_res2.branch2.c_bn.bias: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2a_bn_s: (256,) => s4.pathway0_res3.branch2.a_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2b_bn_riv: (256,) => s4.pathway0_res3.branch2.b_bn.running_var: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_2_branch2c_bn_riv: (1024,) => s4.pathway0_res2.branch2.c_bn.running_var: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2b_w: (128, 128, 1, 3, 3) => s3.pathway0_res2.branch2.b.weight: (128, 128, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2a_w: (64, 256, 3, 1, 1) => s2.pathway0_res1.branch2.a.weight: (64, 256, 3, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch1_w: (1024, 512, 1, 1, 1) => s4.pathway0_res0.branch1.weight: (1024, 512, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2b_bn_riv: (128,) => s3.pathway0_res2.branch2.b_bn.running_var: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res5.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_0_branch1_bn_rm: (256,) => s2.pathway0_res0.branch1_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2b_bn_s: (64,) => s2.pathway0_res2.branch2.b_bn.weight: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res_conv1_bn_rm: (64,) => s1.pathway0_stem.bn.running_mean: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2b_bn_rm: (128,) => s3.pathway0_res2.branch2.b_bn.running_mean: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2b_bn_b: (64,) => s2.pathway0_res2.branch2.b_bn.bias: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2c_bn_s: (256,) => s2.pathway0_res2.branch2.c_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch1_bn_rm: (1024,) => s4.pathway0_res0.branch1_bn.running_mean: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch1_bn_riv: (1024,) => s4.pathway0_res0.branch1_bn.running_var: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch1_bn_s: (2048,) => s5.pathway0_res0.branch1_bn.weight: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res0.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2b_bn_b: (64,) => s2.pathway0_res1.branch2.b_bn.bias: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch2c_bn_rm: (2048,) => s5.pathway0_res0.branch2.c_bn.running_mean: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res5_0_branch1_bn_b: (2048,) => s5.pathway0_res0.branch1_bn.bias: (2048,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_0_branch2a_w: (256, 512, 3, 1, 1) => s4.pathway0_res0.branch2.a.weight: (256, 512, 3, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_4_branch2c_bn_rm: (1024,) => s4.pathway0_res4.branch2.c_bn.running_mean: (1024,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2b_bn_s: (64,) => s2.pathway0_res1.branch2.b_bn.weight: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch1_bn_riv: (512,) => s3.pathway0_res0.branch1_bn.running_var: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2a_bn_rm: (256,) => s4.pathway0_res1.branch2.a_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_0_branch2c_bn_riv: (512,) => s3.pathway0_res0.branch2.c_bn.running_var: (512,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2a_w: (128, 512, 1, 1, 1) => s3.pathway0_res1.branch2.a.weight: (128, 512, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2b_w: (64, 64, 1, 3, 3) => s2.pathway0_res1.branch2.b.weight: (64, 64, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_1_branch2c_w: (512, 128, 1, 1, 1) => s3.pathway0_res1.branch2.c.weight: (512, 128, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_2_branch2a_bn_riv: (128,) => s3.pathway0_res2.branch2.a_bn.running_var: (128,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_2_branch2c_bn_rm: (256,) => s2.pathway0_res2.branch2.c_bn.running_mean: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  204 - \u001b[1;30mWARNING\u001b[0m - \u001b[33m!! pred_b: (400,) does not match head.projection.bias: (51,)\u001b[0m\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res3_3_branch2b_w: (128, 128, 1, 3, 3) => s3.pathway0_res3.branch2.b.weight: (128, 128, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res2_1_branch2b_bn_riv: (64,) => s2.pathway0_res1.branch2.b_bn.running_var: (64,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  204 - \u001b[1;30mWARNING\u001b[0m - \u001b[33m!! pred_w: (400, 2048) does not match head.projection.weight: (51, 2048)\u001b[0m\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2a_w: (256, 1024, 1, 1, 1) => s4.pathway0_res3.branch2.a.weight: (256, 1024, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_1_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res1.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_3_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res3.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2a_bn_s: (256,) => s4.pathway0_res5.branch2.a_bn.weight: (256,)\n",
      "\u001b[34mpyvideoai.slowfast.utils.checkpoint:\u001b[0m  195 - \u001b[1;30mINFO\u001b[0m - res4_5_branch2a_bn_b: (256,) => s4.pathway0_res5.branch2.a_bn.bias: (256,)\n",
      "\u001b[34mpyvideoai.tasks.task:\u001b[0m   23 - \u001b[1;30mINFO\u001b[0m - cfg.get_criterions not defined. Using {'train': CrossEntropyLoss(), 'val': CrossEntropyLoss()}\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  350 - \u001b[1;30mINFO\u001b[0m - ReduceLROnPlateauMultiple scheduler is selected. The `scheduler.step(val_loss, val_best_metric)` function will be called at the end of epoch (after validation), but not every iteration.\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  413 - \u001b[1;30mINFO\u001b[0m - use_amp=True and this will speed up the training. If you encounter an error, consider updating the model to support AMP or setting this to False.\n",
      "\u001b[34mpyvideoai.metrics.metric:\u001b[0m  465 - \u001b[1;30mINFO\u001b[0m - best_metric returns multiple metric values and PyVideoAI will use the first one: val_acc.\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  496 - \u001b[1;30mINFO\u001b[0m - Training for maximum of 200 epochs\n",
      "\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  533 - \u001b[1;30mINFO\u001b[0m - Epoch 0/199\n",
      "\u001b[34mpyvideoai.train_and_eval:\u001b[0m  240 - \u001b[1;30mINFO\u001b[0m -  Train Iter:  223/ 223 - Sample:   3568/  3570 - 105s - lr: 0.00016000 - loss: 3.8112 - acc: 0.1037                            \n",
      "\u001b[34mpyvideoai.train_and_eval:\u001b[0m  462 - \u001b[1;30mINFO\u001b[0m -  One-clip Eval Iter:   96/  96 - Sample:   1530/  1530 - 20s - val_loss: 3.9168 - val_acc: 0.3131 - val_acc_top5: 0.5386        \n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  689 - \u001b[1;30mINFO\u001b[0m - Saving model to /mnt/hdd/kiyoon/PyVideoAI_experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_000/weights/epoch_0000.pth\n",
      "\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  533 - \u001b[1;30mINFO\u001b[0m - Epoch 1/199\n",
      "\u001b[34mpyvideoai.train_and_eval:\u001b[0m  240 - \u001b[1;30mINFO\u001b[0m -  Train Iter:  223/ 223 - Sample:   3568/  3570 - 105s - lr: 0.00016000 - loss: 3.4382 - acc: 0.3192                            \n",
      "\u001b[34mpyvideoai.train_and_eval:\u001b[0m  462 - \u001b[1;30mINFO\u001b[0m -  One-clip Eval Iter:   96/  96 - Sample:   1530/  1530 - 20s - val_loss: 3.8559 - val_acc: 0.4170 - val_acc_top5: 0.6948        \n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  689 - \u001b[1;30mINFO\u001b[0m - Saving model to /mnt/hdd/kiyoon/PyVideoAI_experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_000/weights/epoch_0001.pth\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  709 - \u001b[1;30mINFO\u001b[0m - Removing previous model: /mnt/hdd/kiyoon/PyVideoAI_experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_000/weights/epoch_0000.pth\n",
      "\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  533 - \u001b[1;30mINFO\u001b[0m - Epoch 2/199\n",
      "\u001b[34mpyvideoai.train_and_eval:\u001b[0m  240 - \u001b[1;30mINFO\u001b[0m -  Train Iter:  223/ 223 - Sample:   3568/  3570 - 105s - lr: 0.00016000 - loss: 3.0200 - acc: 0.4126                            \n",
      "\u001b[34mpyvideoai.train_and_eval:\u001b[0m  462 - \u001b[1;30mINFO\u001b[0m -  One-clip Eval Iter:   96/  96 - Sample:   1530/  1530 - 20s - val_loss: 3.7680 - val_acc: 0.4817 - val_acc_top5: 0.7562        \n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  689 - \u001b[1;30mINFO\u001b[0m - Saving model to /mnt/hdd/kiyoon/PyVideoAI_experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_000/weights/epoch_0002.pth\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  709 - \u001b[1;30mINFO\u001b[0m - Removing previous model: /mnt/hdd/kiyoon/PyVideoAI_experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_000/weights/epoch_0001.pth\n",
      "\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  533 - \u001b[1;30mINFO\u001b[0m - Epoch 3/199\n",
      "\u001b[34mpyvideoai.train_and_eval:\u001b[0m  240 - \u001b[1;30mINFO\u001b[0m -  Train Iter:  223/ 223 - Sample:   3568/  3570 - 106s - lr: 0.00016000 - loss: 2.6647 - acc: 0.4675                            \n",
      "\u001b[34mpyvideoai.train_and_eval:\u001b[0m  462 - \u001b[1;30mINFO\u001b[0m -  One-clip Eval Iter:   96/  96 - Sample:   1530/  1530 - 20s - val_loss: 3.7077 - val_acc: 0.5229 - val_acc_top5: 0.8078        \n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  689 - \u001b[1;30mINFO\u001b[0m - Saving model to /mnt/hdd/kiyoon/PyVideoAI_experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_000/weights/epoch_0003.pth\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  709 - \u001b[1;30mINFO\u001b[0m - Removing previous model: /mnt/hdd/kiyoon/PyVideoAI_experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_000/weights/epoch_0002.pth\n",
      "\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  533 - \u001b[1;30mINFO\u001b[0m - Epoch 4/199\n",
      "\u001b[34mpyvideoai.train_and_eval:\u001b[0m  240 - \u001b[1;30mINFO\u001b[0m -  Train Iter:  223/ 223 - Sample:   3568/  3570 - 106s - lr: 0.00016000 - loss: 2.3763 - acc: 0.5107                            \n",
      "\u001b[34mpyvideoai.train_and_eval:\u001b[0m  462 - \u001b[1;30mINFO\u001b[0m -  One-clip Eval Iter:   96/  96 - Sample:   1530/  1530 - 20s - val_loss: 3.6506 - val_acc: 0.5451 - val_acc_top5: 0.8340        \n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  689 - \u001b[1;30mINFO\u001b[0m - Saving model to /mnt/hdd/kiyoon/PyVideoAI_experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_000/weights/epoch_0004.pth\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  709 - \u001b[1;30mINFO\u001b[0m - Removing previous model: /mnt/hdd/kiyoon/PyVideoAI_experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_000/weights/epoch_0003.pth\n",
      "\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  533 - \u001b[1;30mINFO\u001b[0m - Epoch 5/199\n",
      "\u001b[34mpyvideoai.train_and_eval:\u001b[0m  240 - \u001b[1;30mINFO\u001b[0m -  Train Iter:  223/ 223 - Sample:   3568/  3570 - 106s - lr: 0.00016000 - loss: 2.1529 - acc: 0.5575                            \n",
      "\u001b[34mpyvideoai.train_and_eval:\u001b[0m  462 - \u001b[1;30mINFO\u001b[0m -  One-clip Eval Iter:   96/  96 - Sample:   1530/  1530 - 20s - val_loss: 3.6211 - val_acc: 0.5902 - val_acc_top5: 0.8601        \n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  689 - \u001b[1;30mINFO\u001b[0m - Saving model to /mnt/hdd/kiyoon/PyVideoAI_experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_000/weights/epoch_0005.pth\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  709 - \u001b[1;30mINFO\u001b[0m - Removing previous model: /mnt/hdd/kiyoon/PyVideoAI_experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_000/weights/epoch_0004.pth\n",
      "\n",
      "\u001b[34mpyvideoai.train_multiprocess:\u001b[0m  533 - \u001b[1;30mINFO\u001b[0m - Epoch 6/199\n",
      "-----------------------------------------------------------\n",
      "----------OUTPUT SIMPLIFIED FOR NOTEBOOK VIEWERS-----------\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "!\"$PYVIDEOAI_DIR/tools/run_singlenode.sh\" train 1 -D hmdb -M i3d_resnet50 -E crop224_8x8_largejit_plateau_1scrop5tcrop_split1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ff61a-551c-40b7-9928-a80f91fdb7fc",
   "metadata": {},
   "source": [
    "### Saving model weights\n",
    "It will only keep the models with peak validation accuracy, plus the last epoch. That's why you see that it's removing the previous model when it's not better than current.  \n",
    "This is due to the command line argument `--save_mode last_and_peaks` which is set by default.  \n",
    "Use `--save_mode all` in order to keep checkpoints from every epoch.  \n",
    "\n",
    "### Experiment output structure\n",
    "The output directory will be organised as follows:  \n",
    "\n",
    "```\n",
    "${experiment_root}\n",
    "└── ${dataset}\n",
    "    └── ${model_name}\n",
    "        └── ${experiment_name}\n",
    "                └── ${subfolder_name}\n",
    "                    └── version_000\n",
    "                        ├── configs\n",
    "                        │   └── args.json\n",
    "                        ├── logs\n",
    "                        │   └── summary.csv\n",
    "                        ├── plots\n",
    "                        │   ├── accuracy.pdf\n",
    "                        │   ├── loss.pdf\n",
    "                        │   ├── video_accuracy_top1.pdf\n",
    "                        │   └── video_accuracy_top5.pdf\n",
    "                        ├── (predictions)\n",
    "                        ├── tensorboard_runs\n",
    "                        └── weights\n",
    "                            ├── epoch_0000.pth\n",
    "                            └── epoch_0001.pth\n",
    "```\n",
    "\n",
    "You can define `${subfolder_name}` by adding `-S` argument. By default, there will be no subfolder.\n",
    "\n",
    "\n",
    "### Visualisation\n",
    "#### Telegram\n",
    "If you've set up the Telegram bot, it will report the training stats (and even get notifications of errors running the script) like this:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/12980409/122335586-7cb10a80-cf76-11eb-950f-af08c20055d4.png\" alt=\"Telegram bot stat report example\" width=\"400\">\n",
    "\n",
    "Not satisfied with the looks of the plots and message? Don't worry, you can customise them easily.  \n",
    "If the exp_config file doesn't define `telegram_reporter`, the default is as follows:\n",
    "```python\n",
    "from pyvideoai.visualisations.telegram_reporter import DefaultTelegramReporter\n",
    "telegram_reporter = DefaultTelegramReporter()\n",
    "```\n",
    "\n",
    "Copy `pyvideoai/visualisations/telegram_reporter.py` and change it as you want. Then, use your custom telegram reporter by adding few lines in the exp_config like below.  \n",
    "```python\n",
    "from my_telegram_reporter import MyTelegramReporter\n",
    "telegram_reporter = MyTelegramReporter()\n",
    "```\n",
    "\n",
    "#### TensorBoard\n",
    "You can use TensorBoard in `data/experiments/hmdb/i3d_resnet/crop224_8x8_largejit_steplr_1scrop5tcrop_split1/tensorboard_runs`.  \n",
    "[Example tensorboard link](https://tensorboard.dev/experiment/mGSBcdZfQmWJNd658zHLbQ)\n",
    "\n",
    "### Early Stopping\n",
    "The training will stop when the validation loss and accuracy saturate for 20 epochs.  \n",
    "See `early_stopping_condition()` in the exp_config."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec71f3-d524-4ca6-af49-4af6d4c9eb72",
   "metadata": {},
   "source": [
    "## Evaluate using the saved weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb7f31-dbb6-486c-97a8-99de40fa1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -p will save the predictions into a pickle file.\n",
    "# -l -2 will pick the best model (highest validation accuracy).\n",
    "# -l -1 will pick the last model\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "!\"$PYVIDEOAI_DIR/tools/run_singlenode.sh\" eval 1 -D hmdb -M i3d_resnet50 -E crop224_8x8_largejit_plateau_1scrop5tcrop_split1 -l -2 -p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb811fc-9c96-4ec5-ae1f-31b2d021451b",
   "metadata": {},
   "source": [
    "## Resume training\n",
    "\n",
    "### From the last checkpoint\n",
    "By simply adding `-l -1`, it will resume from the last checkpoint. `-l -2` means to load from the best checkpoint.   \n",
    "Or, `-l 50` by resuming from 50th epoch's checkpoint (and start from 51st epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ae7ad-65e1-439d-a998-7c7d368e7a1b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env VAI_NUM_EPOCHS=1000\n",
    "!\"$PYVIDEOAI_DIR/tools/run_singlenode.sh\" train 1 -D hmdb -M i3d_resnet50 -E crop224_8x8_largejit_plateau_1scrop5tcrop_split1 -l -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536fb9c-bcdb-47a2-824a-ce97f9dc0bac",
   "metadata": {},
   "source": [
    "***\n",
    "# Running TSN / TRN / TSM\n",
    "\n",
    "Let's train using different model with the same dataset.  \n",
    "\n",
    "\n",
    "### Dense sampling vs Sparse sampling\n",
    "The difference between the I3D and the TSN/TRN/TSM is that the former is sampling videos densely, and the latter is sparsely.  \n",
    "Note the different dataloader in `i3d_resnet50-crop224_8x8_largejit_plateau_1scrop5tcrop_split1.py` and `tsm_resnet50-crop224_8frame_largejit_plateau_5scrop_split1.py` in `exp_configs/hmdb`.  \n",
    "\n",
    "\n",
    "### LR policy\n",
    "The new exp_config has an optimiser policy. Refer to `get_optim_policies(model)` in the config file.  \n",
    "This lets you set different learning rate for different layers in the model.\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66708ef-fcda-4a81-abb8-3d9acc425853",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "!\"$PYVIDEOAI_DIR/tools/run_singlenode.sh\" train 1 -D hmdb -M tsm_resnet50 -E crop224_8frame_largejit_plateau_5scrop_split1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e226b26-0958-4efd-9c39-1beee47e480a",
   "metadata": {},
   "source": [
    "## Changing dataset or model\n",
    "To change dataset, simply change the folder.  \n",
    "This changes the dataset_config to `dataset_configs/something_v1.py`, but keep other settings unchanged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3cdce0c-7cbe-42f3-af51-665d274b978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can replace `cp` to `ln -s` if you want to link the two files.\n",
    "!cp \"$PYVIDEOAI_DIR/exp_configs/hmdb/tsm_resnet50-crop224_8frame_largejit_plateau_5scrop_split1.py\" \"$PYVIDEOAI_DIR/exp_configs/something_v1/tsm_resnet50-crop224_8frame_largejit_plateau_5scrop_split1.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1936aaa-ab3f-4082-acb8-574e94e4a75b",
   "metadata": {},
   "source": [
    "To run another 2D network (TSN), you can simply copy the exp_config.  \n",
    "This changes the model_config to `model_configs/tsn_resnet50.py`, but keep other settings such as dataloader and optimiser unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "665a2f18-51c9-4f06-8fbd-9f2afdd640df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can replace `cp` to `ln -s` if you want to link the two files.\n",
    "!cp \"$PYVIDEOAI_DIR/exp_configs/hmdb/tsm_resnet50-crop224_8frame_largejit_plateau_5scrop_split1.py\" \"$PYVIDEOAI_DIR/exp_configs/hmdb/tsn_resnet50-crop224_8frame_largejit_plateau_5scrop_split1.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd9d9a-cebf-4603-8b14-0bf4b9d37343",
   "metadata": {},
   "source": [
    "### Importing from another config file\n",
    "\n",
    "Okay, cool. But is copying the only way to change experiment settings? What if you have 1000 experiments and later they're too difficult to explore what the changed settings are?\n",
    "\n",
    "Indeed. Copying too many codes is the easiest way to make mistakes.  \n",
    "The other way is to \"import\" the other config file and change only the part of the configuration.  \n",
    "However, normal Python importing wouldn't work.\n",
    "\n",
    "```python\n",
    "# Example exp_config that loads config from another one.\n",
    "# Good idea, but would NOT work.\n",
    "from tsm_resnet50-crop224_8frame_largejit_plateau_5scrop_split1 import *\n",
    "input_frame_length = 32\n",
    "```\n",
    "\n",
    "Instead, use `_exec_relative_(relpath)`.  \n",
    "Open a new config file like `exp_configs/hmdb/tsm_resnet50-32frame.py` in an editor and try below.  \n",
    "```python\n",
    "# Example exp_config that loads config from another one. Keep all settings except the number of frames.\n",
    "# Right way\n",
    "_exec_relative_('tsm_resnet50-crop224_8frame_largejit_plateau_5scrop_split1.py')\n",
    "\n",
    "input_frame_length = 32\n",
    "```\n",
    "\n",
    "### Change config without copying. Use one config for all!\n",
    "\n",
    "What if your task is to find the best hyperparameter and you want to try 1000 different learning rates?  \n",
    "Do you create 1000 config files and write learning rate manually?  \n",
    "Here's a better way to do so: use ENVIRONMENT VARIABLES!\n",
    "\n",
    "Notice that the learning rate in the exp_config is defined as\n",
    "```python\n",
    "base_learning_rate = float(os.getenv('VAI_BASE_LR', 1e-5))\n",
    "```\n",
    "in `optimiser()`. Thus, you can overwrite the learning rate by setting the `BASE_LR` environment variable, or the default will be 1e-5.\n",
    "\n",
    "Your bash script may look like:\n",
    "```bash\n",
    "for LR in {1..10}    # 1e-1, 1e-2, ..., 1e-10\n",
    "do\n",
    "    BASE_LR=1e-$LR CUDA_VISIBLE_DEVICES=0 ~/PyVideoAI/tools/run_singlenode.sh train 1 \\\n",
    "    -D hmdb -M tsm_resnet50 -E crop224_8frame_largejit_plateau_5scrop_split1 \\\n",
    "    -S ~/experiments/base_lr-1e-$LR\n",
    "done\n",
    "```\n",
    "\n",
    "`-S` argument creates a subfolder named `base_lr-1e-10` for example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831d0ab-e95a-4a94-830c-682b9134a26e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
