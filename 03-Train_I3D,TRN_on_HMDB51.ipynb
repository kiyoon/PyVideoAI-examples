{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44eb80d-bdd5-4f1f-954f-c8c10301d679",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now the fun part begins. **Let's train the I3D network on the HMDB-51 dataset!**  \n",
    "\n",
    "First, we want to download the pre-trained weights from the [MODEL_ZOO.md](https://github.com/kiyoon/PyVideoAI/blob/master/MODEL_ZOO.md)  \n",
    "We'll use the I3D pretrained on the Kinetics-400 dataset, with 8-frame input.  \n",
    "\n",
    "Note that the path to the pretrained weights is defined in `model_configs/i3d_resnet50.py` as below.  \n",
    "```python\n",
    "kinetics400_pretrained_path_8x8 = os.path.join(DATA_DIR, 'pretrained', 'kinetics400/i3d_resnet50/I3D_8x8_R50.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b00c0c1-2cb7-41fa-aff9-916f779a8cda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYVIDEOAI_DIR=/home/kiyoon/project/PyVideoAI\n",
      "env: DATA_DIR=/home/kiyoon/project/PyVideoAI/data\n",
      "env: HDD_PATH=/fast/kiyoon\n"
     ]
    }
   ],
   "source": [
    "## IMPORTANT: You must change path values in `00-storage_location.py` before executing below.\n",
    "# Environments for future use\n",
    "\n",
    "from pyvideoai.config import PYVIDEOAI_DIR, DATA_DIR\n",
    "%env PYVIDEOAI_DIR=$PYVIDEOAI_DIR\n",
    "%env DATA_DIR=$DATA_DIR\n",
    "\n",
    "import os\n",
    "exec(open(os.path.join(PYVIDEOAI_DIR, 'examples', '00-storage_location.py')).read())\n",
    "%env HDD_PATH=$HDD_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6efdac-059c-46d4-a419-8856d6b8c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-14 03:17:54--  https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/I3D_8x8_R50.pkl\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 224598662 (214M) [application/octet-stream]\n",
      "Saving to: ‘/home/kiyoon/project/PyVideoAI/data/pretrained/I3D_8x8_R50.pkl’\n",
      "\n",
      "I3D_8x8_R50.pkl     100%[===================>] 214.19M  11.4MB/s    in 20s     \n",
      "\n",
      "2021-06-14 03:18:15 (10.6 MB/s) - ‘/home/kiyoon/project/PyVideoAI/data/pretrained/I3D_8x8_R50.pkl’ saved [224598662/224598662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Link the pretrained weight directory to HDD\n",
    "!mkdir -p \"$HDD_PATH/pretrained/kinetics400/i3d_resnet50\"\n",
    "!ln -s \"$HDD_PATH/pretrained\" \"$DATA_DIR/\"\n",
    "\n",
    "# Download\n",
    "!wget https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/I3D_8x8_R50.pkl -P \"$DATA_DIR/pretrained/kinetics400/i3d_resnet50\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c762f-6e2b-4841-91c3-82a299e00600",
   "metadata": {},
   "source": [
    "Now we have our datasets processed, and the pretrained weights ready.  \n",
    "We want to use three config files.  \n",
    "- `hmdb.py` in `dataset_configs`,  \n",
    "- `i3d_resnet50.py` in `model_configs`,  \n",
    "- `hmdb/i3d_resnet50-crop224_8x8_largejit_plateau_1scrop5tcrop_split1.py` in `exp_configs`.\n",
    "\n",
    "NOTE: The only difference between the inference example is the `load_pretrained(model)` function definition in the exp_config file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790f1cd2-a928-45fa-8bc6-3b9058d2f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Setup Telegram bot to report the experiment stats\n",
    "import os\n",
    "if not os.path.isfile(f\"{PYVIDEOAI_DIR}/tools/key.ini\"):\n",
    "    !cp \"$PYVIDEOAI_DIR/tools/key.ini\"{.template,}\n",
    "# EDIT the `tools/key.ini` file on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bf4ff2-68f2-4f49-a76a-eef25f45ef8a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.tasks.task:   35 - INFO - cfg.last_activation not defined. Using softmax\n",
      "experiment_utils.experiment_builder:  170 - INFO - Telegram bot initialised with keys in /home/kiyoon/project/PyVideoAI/tools/key.ini and using the bot number 0\n",
      "pyvideoai.train_multiprocess:  136 - INFO - PyTorch==1.8.1\n",
      "pyvideoai.train_multiprocess:  137 - INFO - PyVideoAI==v0.1+178.gddcd26d.dirty\n",
      "pyvideoai.train_multiprocess:  138 - INFO - Experiment folder: /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003 on host aislab-2\n",
      "pyvideoai.train_multiprocess:  142 - INFO - args: {\n",
      "    \"local_world_size\": 1,\n",
      "    \"shard_id\": 0,\n",
      "    \"num_shards\": 1,\n",
      "    \"init_method\": \"tcp://localhost:19999\",\n",
      "    \"backend\": \"nccl\",\n",
      "    \"num_epochs\": 100,\n",
      "    \"experiment_root\": \"/fast/kiyoon/experiments\",\n",
      "    \"dataset\": \"hmdb\",\n",
      "    \"model\": \"i3d_resnet50\",\n",
      "    \"experiment_name\": \"crop224_8x8_largejit_plateau_1scrop5tcrop_split1\",\n",
      "    \"dataset_channel\": null,\n",
      "    \"model_channel\": null,\n",
      "    \"experiment_channel\": null,\n",
      "    \"save_mode\": \"last_and_peaks\",\n",
      "    \"training_speed\": \"standard\",\n",
      "    \"load_epoch\": null,\n",
      "    \"seed\": 12,\n",
      "    \"multi_crop_val_period\": -1,\n",
      "    \"telegram_post_period\": 10,\n",
      "    \"telegram_bot_idx\": 0,\n",
      "    \"dataloader_num_workers\": 4,\n",
      "    \"refresh_period\": 1,\n",
      "    \"version\": \"auto\"\n",
      "}\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  101 - INFO - Constructing video dataset train...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  146 - INFO - Constructing video dataloader (size: 3570) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/train1.csv\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  101 - INFO - Constructing video dataset test...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  146 - INFO - Constructing video dataloader (size: 1530) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/test1.csv\n",
      "pyvideoai.train_multiprocess:  174 - INFO - Using batch size of 64 per process (per GPU), resulting in total size of 64.\n",
      "pyvideoai.train_multiprocess:  175 - INFO - Using validation batch size of 64 per process (per GPU), resulting in total size of 64.\n",
      "pyvideoai.utils.misc:   52 - INFO - Model:\n",
      "ResNetModel(\n",
      "  (s1): VideoModelStem(\n",
      "    (pathway0_stem): ResNetBasicStem(\n",
      "      (conv): Conv3d(3, 64, kernel_size=[5, 7, 7], stride=[1, 2, 2], padding=[2, 3, 3], bias=False)\n",
      "      (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (pool_layer): MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "-----------------------------------------------------------\n",
      "----------OUTPUT SIMPLIFIED FOR NOTEBOOK VIEWERS-----------\n",
      "-----------------------------------------------------------\n",
      "  (head): ResNetBasicHead(\n",
      "    (pathway0_avgpool): AvgPool3d(kernel_size=[4, 7, 7], stride=1, padding=0)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (projection): Linear(in_features=2048, out_features=51, bias=True)\n",
      "    (act): Softmax(dim=4)\n",
      "  )\n",
      ")\n",
      "pyvideoai.utils.misc:   53 - INFO - Params: 27,328,371\n",
      "pyvideoai.utils.misc:   54 - INFO - Mem: 104.4853515625 MiB\n",
      "pyvideoai.utils.misc:   55 - INFO - nvidia-smi\n",
      "pyvideoai.utils.misc:   56 - INFO - Wed Aug  4 10:51:46 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3090    Off  | 00000000:23:00.0 Off |                  N/A |\n",
      "|  0%   52C    P2   120W / 370W |   1213MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 3090    Off  | 00000000:2D:00.0 Off |                  N/A |\n",
      "| 55%   61C    P0    47W / 370W |      0MiB / 24251MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2830129      C   ...3/envs/videoai/bin/python     1211MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res_conv1_bn_b: (64,) => s1.pathway0_stem.bn.bias: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res_conv1_bn_s: (64,) => s1.pathway0_stem.bn.weight: (64,)\n",
      "-----------------------------------------------------------\n",
      "----------OUTPUT SIMPLIFIED FOR NOTEBOOK VIEWERS-----------\n",
      "-----------------------------------------------------------\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res1.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res3.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2a_bn_s: (256,) => s4.pathway0_res5.branch2.a_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2a_bn_b: (256,) => s4.pathway0_res5.branch2.a_bn.bias: (256,)\n",
      "pyvideoai.tasks.task:   17 - INFO - cfg.criterion not defined. Using CrossEntropyLoss()\n",
      "pyvideoai.train_multiprocess:  300 - INFO - ReduceLROnPlateauMultiple scheduler is selected. The `scheduler.step(val_loss, val_best_metric)` function will be called at the end of epoch (after validation), but not every iteration.\n",
      "pyvideoai.train_multiprocess:  363 - INFO - use_amp=True and this will speed up the training. If you encounter an error, consider updating the model to support AMP or setting this to False.\n",
      "pyvideoai.train_multiprocess:  467 - INFO - Epoch 0/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:   55/  55 - Sample:   3520/  3570 - ETA:    0s - lr: 0.00064000 - batch_loss: 3.7140 - loss: 3.8231 - acc: 0.0957\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  201 - INFO -  Train Iter:   55/  55 - Sample:   3520/  3570 - 83s - lr: 0.00064000 - loss: 3.8231 - acc: 0.0957                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.9180 - val_acc: 0.3190\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  421 - INFO -  One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - 26s - val_loss: 3.9180 - val_acc: 0.3190        \n",
      "pyvideoai.train_multiprocess:  600 - INFO - Saving model to /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0000.pth\n",
      "pyvideoai.train_multiprocess:  467 - INFO - Epoch 1/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:   55/  55 - Sample:   3520/  3570 - ETA:    0s - lr: 0.00064000 - batch_loss: 3.2681 - loss: 3.3617 - acc: 0.3341\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  201 - INFO -  Train Iter:   55/  55 - Sample:   3520/  3570 - 78s - lr: 0.00064000 - loss: 3.3617 - acc: 0.3341                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.8438 - val_acc: 0.4046\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  421 - INFO -  One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - 25s - val_loss: 3.8438 - val_acc: 0.4046        \n",
      "pyvideoai.train_multiprocess:  600 - INFO - Saving model to /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0001.pth\n",
      "pyvideoai.train_multiprocess:  620 - INFO - Removing previous model: /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0000.pth\n",
      "pyvideoai.train_multiprocess:  467 - INFO - Epoch 2/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:   55/  55 - Sample:   3520/  3570 - ETA:    0s - lr: 0.00064000 - batch_loss: 2.5326 - loss: 2.8910 - acc: 0.4182\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  201 - INFO -  Train Iter:   55/  55 - Sample:   3520/  3570 - 77s - lr: 0.00064000 - loss: 2.8910 - acc: 0.4182                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.7692 - val_acc: 0.4902\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  421 - INFO -  One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - 25s - val_loss: 3.7692 - val_acc: 0.4902        \n",
      "pyvideoai.train_multiprocess:  600 - INFO - Saving model to /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0002.pth\n",
      "pyvideoai.train_multiprocess:  620 - INFO - Removing previous model: /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0001.pth\n",
      "pyvideoai.train_multiprocess:  467 - INFO - Epoch 3/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:   55/  55 - Sample:   3520/  3570 - ETA:    0s - lr: 0.00064000 - batch_loss: 2.3915 - loss: 2.5583 - acc: 0.4804\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  201 - INFO -  Train Iter:   55/  55 - Sample:   3520/  3570 - 77s - lr: 0.00064000 - loss: 2.5583 - acc: 0.4804                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.7197 - val_acc: 0.5314\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  421 - INFO -  One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - 25s - val_loss: 3.7197 - val_acc: 0.5314        \n",
      "pyvideoai.train_multiprocess:  600 - INFO - Saving model to /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0003.pth\n",
      "pyvideoai.train_multiprocess:  620 - INFO - Removing previous model: /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0002.pth\n",
      "pyvideoai.train_multiprocess:  467 - INFO - Epoch 4/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:   55/  55 - Sample:   3520/  3570 - ETA:    0s - lr: 0.00064000 - batch_loss: 2.2755 - loss: 2.2977 - acc: 0.5312\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  201 - INFO -  Train Iter:   55/  55 - Sample:   3520/  3570 - 78s - lr: 0.00064000 - loss: 2.2977 - acc: 0.5312                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.6743 - val_acc: 0.5699\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  421 - INFO -  One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - 25s - val_loss: 3.6743 - val_acc: 0.5699        \n",
      "pyvideoai.train_multiprocess:  600 - INFO - Saving model to /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0004.pth\n",
      "pyvideoai.train_multiprocess:  620 - INFO - Removing previous model: /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0003.pth\n",
      "pyvideoai.train_multiprocess:  467 - INFO - Epoch 5/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:   55/  55 - Sample:   3520/  3570 - ETA:    0s - lr: 0.00064000 - batch_loss: 2.0701 - loss: 2.0678 - acc: 0.5673\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  201 - INFO -  Train Iter:   55/  55 - Sample:   3520/  3570 - 77s - lr: 0.00064000 - loss: 2.0678 - acc: 0.5673                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.6376 - val_acc: 0.6065\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  421 - INFO -  One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - 25s - val_loss: 3.6376 - val_acc: 0.6065        \n",
      "pyvideoai.train_multiprocess:  600 - INFO - Saving model to /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0005.pth\n",
      "pyvideoai.train_multiprocess:  620 - INFO - Removing previous model: /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0004.pth\n",
      "pyvideoai.train_multiprocess:  467 - INFO - Epoch 6/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:   55/  55 - Sample:   3520/  3570 - ETA:    0s - lr: 0.00064000 - batch_loss: 1.7807 - loss: 1.8762 - acc: 0.5974\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  201 - INFO -  Train Iter:   55/  55 - Sample:   3520/  3570 - 77s - lr: 0.00064000 - loss: 1.8762 - acc: 0.5974                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.6092 - val_acc: 0.6216\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  421 - INFO -  One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - 25s - val_loss: 3.6092 - val_acc: 0.6216        \n",
      "pyvideoai.train_multiprocess:  600 - INFO - Saving model to /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0006.pth\n",
      "pyvideoai.train_multiprocess:  620 - INFO - Removing previous model: /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0005.pth\n",
      "pyvideoai.train_multiprocess:  467 - INFO - Epoch 7/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:   55/  55 - Sample:   3520/  3570 - ETA:    0s - lr: 0.00064000 - batch_loss: 1.7178 - loss: 1.7382 - acc: 0.6330\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  201 - INFO -  Train Iter:   55/  55 - Sample:   3520/  3570 - 77s - lr: 0.00064000 - loss: 1.7382 - acc: 0.6330                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.5820 - val_acc: 0.6484\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  421 - INFO -  One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - 25s - val_loss: 3.5820 - val_acc: 0.6484        \n",
      "pyvideoai.train_multiprocess:  600 - INFO - Saving model to /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0007.pth\n",
      "pyvideoai.train_multiprocess:  620 - INFO - Removing previous model: /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0006.pth\n",
      "pyvideoai.train_multiprocess:  467 - INFO - Epoch 8/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:   55/  55 - Sample:   3520/  3570 - ETA:    0s - lr: 0.00064000 - batch_loss: 1.5136 - loss: 1.6054 - acc: 0.6520\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  201 - INFO -  Train Iter:   55/  55 - Sample:   3520/  3570 - 77s - lr: 0.00064000 - loss: 1.6054 - acc: 0.6520                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.5567 - val_acc: 0.6601\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  421 - INFO -  One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - 25s - val_loss: 3.5567 - val_acc: 0.6601        \n",
      "pyvideoai.train_multiprocess:  600 - INFO - Saving model to /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0008.pth\n",
      "pyvideoai.train_multiprocess:  620 - INFO - Removing previous model: /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0007.pth\n",
      "pyvideoai.train_multiprocess:  467 - INFO - Epoch 9/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:   55/  55 - Sample:   3520/  3570 - ETA:    0s - lr: 0.00064000 - batch_loss: 1.3608 - loss: 1.5025 - acc: 0.6551\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  201 - INFO -  Train Iter:   55/  55 - Sample:   3520/  3570 - 77s - lr: 0.00064000 - loss: 1.5025 - acc: 0.6551                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.5405 - val_acc: 0.6667\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  421 - INFO -  One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - 25s - val_loss: 3.5405 - val_acc: 0.6667        \n",
      "pyvideoai.train_multiprocess:  565 - INFO - Sending plots to Telegram.\n",
      "pyvideoai.train_multiprocess:  600 - INFO - Saving model to /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0009.pth\n",
      "pyvideoai.train_multiprocess:  620 - INFO - Removing previous model: /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0008.pth\n",
      "pyvideoai.train_multiprocess:  467 - INFO - Epoch 10/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:   55/  55 - Sample:   3520/  3570 - ETA:    0s - lr: 0.00064000 - batch_loss: 1.6279 - loss: 1.4124 - acc: 0.6832\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  201 - INFO -  Train Iter:   55/  55 - Sample:   3520/  3570 - 77s - lr: 0.00064000 - loss: 1.4124 - acc: 0.6832                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.5235 - val_acc: 0.6758\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_eval:  421 - INFO -  One-clip Eval Iter:   24/  24 - Sample:   1530/  1530 - 25s - val_loss: 3.5235 - val_acc: 0.6758        \n",
      "pyvideoai.train_multiprocess:  600 - INFO - Saving model to /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0010.pth\n",
      "pyvideoai.train_multiprocess:  620 - INFO - Removing previous model: /fast/kiyoon/experiments/hmdb/i3d_resnet50/crop224_8x8_largejit_plateau_1scrop5tcrop_split1/version_003/weights/epoch_0009.pth\n",
      "pyvideoai.train_multiprocess:  467 - INFO - Epoch 11/99\n",
      "-----------------------------------------------------------\n",
      "----------OUTPUT SIMPLIFIED FOR NOTEBOOK VIEWERS-----------\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%run \"$PYVIDEOAI_DIR/tools/run_train.py\" --local_world_size 1 -D hmdb -M i3d_resnet50 -E crop224_8x8_largejit_plateau_1scrop5tcrop_split1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ff61a-551c-40b7-9928-a80f91fdb7fc",
   "metadata": {},
   "source": [
    "### Saving model weights\n",
    "It will only keep the models with peak validation accuracy, plus the last epoch. That's why you see that it's removing the previous model when it's not better than current.  \n",
    "This is due to the command line argument `--save_mode last_and_peaks` which is set by default.  \n",
    "Use `--save_mode all` in order to keep checkpoints from every epoch.  \n",
    "\n",
    "### Experiment output structure\n",
    "The output directory will be organised as follows:  \n",
    "\n",
    "```\n",
    "${experiment_root}\n",
    "└── ${dataset}\n",
    "    └── ${model_name}\n",
    "        └── ${experiment_name}\n",
    "            └── version_000\n",
    "                ├── configs\n",
    "                │   └── args.json\n",
    "                ├── logs\n",
    "                │   └── summary.csv\n",
    "                ├── plots\n",
    "                │   ├── accuracy.pdf\n",
    "                │   ├── loss.pdf\n",
    "                │   ├── video_accuracy_top1.pdf\n",
    "                │   └── video_accuracy_top5.pdf\n",
    "                ├── (predictions)\n",
    "                ├── tensorboard_runs\n",
    "                └── weights\n",
    "                    ├── epoch_0000.pth\n",
    "                    └── epoch_0001.pth\n",
    "```\n",
    "\n",
    "\n",
    "### Visualisation\n",
    "#### Telegram\n",
    "If you've set up the Telegram bot, it will report the training stats (and even get notifications of errors running the script) like this:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/12980409/122335586-7cb10a80-cf76-11eb-950f-af08c20055d4.png\" alt=\"Telegram bot stat report example\" width=\"400\">\n",
    "\n",
    "Not satisfied with the looks of the plots and message? Don't worry, you can customise them easily.  \n",
    "If the exp_config file doesn't define `telegram_reporter`, the default is as follows:\n",
    "```python\n",
    "from pyvideoai.visualisations.telegram_reporter import DefaultTelegramReporter\n",
    "telegram_reporter = DefaultTelegramReporter()\n",
    "```\n",
    "\n",
    "Copy `pyvideoai/visualisations/telegram_reporter.py` and change it as you want. Then, use your custom telegram reporter by adding few lines in the exp_config like below.  \n",
    "```python\n",
    "from my_telegram_reporter import MyTelegramReporter\n",
    "telegram_reporter = MyTelegramReporter()\n",
    "```\n",
    "\n",
    "#### TensorBoard\n",
    "You can use TensorBoard in `data/experiments/hmdb/i3d_resnet/crop224_8x8_largejit_steplr_1scrop5tcrop_split1/tensorboard_runs`.  \n",
    "[Example tensorboard link](https://tensorboard.dev/experiment/mGSBcdZfQmWJNd658zHLbQ)\n",
    "\n",
    "### Early Stopping\n",
    "The training will stop when the validation loss and accuracy saturate for 20 epochs.  \n",
    "See `early_stopping_condition()` in the exp_config."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec71f3-d524-4ca6-af49-4af6d4c9eb72",
   "metadata": {},
   "source": [
    "## Evaluate using the saved weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb7f31-dbb6-486c-97a8-99de40fa1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -p will save the predictions into a pickle file.\n",
    "# -l -2 will pick the best model (highest validation accuracy).\n",
    "# -l -1 will pick the last model\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%run \"$PYVIDEOAI_DIR/tools/run_eval.py\" --local_world_size 1 -D hmdb -M i3d_resnet50 -E crop224_8x8_largejit_plateau_1scrop5tcrop_split1 -l -2 -p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb811fc-9c96-4ec5-ae1f-31b2d021451b",
   "metadata": {},
   "source": [
    "## Resume training\n",
    "\n",
    "### From the last checkpoint\n",
    "By simply adding `-l -1`, it will resume from the last checkpoint.  \n",
    "Or, `-l 50` by resuming from 50th epoch's checkpoint (and start from 51st epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "274ae7ad-65e1-439d-a998-7c7d368e7a1b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment_utils.experiment_builder:  103 - INFO - Telegram bot initialised with keys in /home/kiyoon/project/PyVideoAI/tools/key.ini and using the bot number 0\n",
      "pyvideoai.train_multiprocess:  121 - INFO - git hash: 48a5d20dc2e17fcd7b4343cc562f043b9839d6e2\n",
      "pyvideoai.train_multiprocess:  125 - INFO - args: {\n",
      "    \"local_world_size\": 1,\n",
      "    \"shard_id\": 0,\n",
      "    \"num_shards\": 1,\n",
      "    \"init_method\": \"tcp://localhost:19999\",\n",
      "    \"backend\": \"nccl\",\n",
      "    \"num_epochs\": 200,\n",
      "    \"experiment_root\": \"/home/kiyoon/project/PyVideoAI/data/experiments\",\n",
      "    \"dataset\": \"hmdb\",\n",
      "    \"model\": \"i3d_resnet50\",\n",
      "    \"experiment_name\": \"crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1\",\n",
      "    \"dataset_channel\": null,\n",
      "    \"model_channel\": null,\n",
      "    \"experiment_channel\": null,\n",
      "    \"save_mode\": \"last_and_peaks\",\n",
      "    \"load_epoch\": -1,\n",
      "    \"seed\": 12,\n",
      "    \"multi_crop_val_freq\": 5,\n",
      "    \"telegram_post_freq\": 5,\n",
      "    \"telegram_bot_idx\": 0,\n",
      "    \"dataloader_num_workers\": 4\n",
      "}\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset train...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 3570) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/train1.csv\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset test...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 1530) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/test1.csv\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset test...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 7650) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/test1.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.utils.misc:   52 - INFO - Model:\n",
      "ResNetModel(\n",
      "  (s1): VideoModelStem(\n",
      "    (pathway0_stem): ResNetBasicStem(\n",
      "      (conv): Conv3d(3, 64, kernel_size=[5, 7, 7], stride=[1, 2, 2], padding=[2, 3, 3], bias=False)\n",
      "      (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (pool_layer): MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "-----------------------------------------------------------\n",
      "----------OUTPUT SIMPLIFIED FOR NOTEBOOK VIEWERS-----------\n",
      "-----------------------------------------------------------\n",
      "  (head): ResNetBasicHead(\n",
      "    (pathway0_avgpool): AvgPool3d(kernel_size=[4, 7, 7], stride=1, padding=0)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (projection): Linear(in_features=2048, out_features=51, bias=True)\n",
      "    (act): Softmax(dim=4)\n",
      "  )\n",
      ")\n",
      "pyvideoai.utils.misc:   53 - INFO - Params: 27,328,371\n",
      "pyvideoai.utils.misc:   54 - INFO - Mem: 4,840.8818359375 MiB\n",
      "pyvideoai.utils.misc:   55 - INFO - nvidia-smi\n",
      "pyvideoai.utils.misc:   56 - INFO - Wed Jun 16 00:02:21 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3090    Off  | 00000000:23:00.0 Off |                  N/A |\n",
      "|  0%   34C    P2    86W / 370W |   6365MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 3090    Off  | 00000000:2D:00.0 Off |                  N/A |\n",
      "| 38%   46C    P0    40W / 370W |      0MiB / 24251MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1353154      C   ...3/envs/videoai/bin/python     6363MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "pyvideoai.utils.loader:   93 - INFO - Loading weights: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0099.pth\n",
      "pyvideoai.train_multiprocess:  220 - INFO - cfg.criterion not defined. Using CrossEntropyLoss()\n",
      "pyvideoai.train_multiprocess:  236 - INFO - cfg.load_optimiser_state not found. By default, it will try to load.\n",
      "pyvideoai.train_multiprocess:  240 - INFO - Loading optimiser state from the checkpoint.\n",
      "pyvideoai.train_multiprocess:  248 - INFO - ReduceLROnPlateau scheduler is selected. The `scheduler.step(val_loss)` function will be called at the end of epoch (after validation), but not every iteration.\n",
      "pyvideoai.train_multiprocess:  250 - INFO - For the ReduceLROnPlateau scheduler, initialise using training stats (summary.csv) instead of the checkpoint.\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 100/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9854 - loss: 0.5750 - batch_acc: 0.6250 - acc: 0.8487        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 95s - lr: 0.00000001 - loss: 0.5750 - acc: 0.8487                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3217 - val_acc: 0.7085        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3217 - val_acc: 0.7085\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0100.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0099.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 101/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "----------OUTPUT SIMPLIFIED FOR NOTEBOOK VIEWERS-----------\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3233 - val_acc: 0.7082 - val_vid_acc_top1: 0.7294 - val_vid_acc_top5: 0.9405\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0199.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0198.pth\n",
      "pyvideoai.train_multiprocess:  455 - SUCCESS - Finished training\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%run \"$PYVIDEOAI_DIR/tools/run_train.py\" --local_world_size 1 -D hmdb -M i3d_resnet50 -E crop224_8x8_largejit_plateau_1scrop5tcrop_split1 -e 200 -l -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536fb9c-bcdb-47a2-824a-ce97f9dc0bac",
   "metadata": {},
   "source": [
    "***\n",
    "# Running TSN / TRN / TSM\n",
    "\n",
    "Let's train using different model with the same dataset.  \n",
    "\n",
    "\n",
    "### Dense sampling vs Sparse sampling\n",
    "The difference between the I3D and the TRN is that the former is sampling videos densely, and the latter is sparsely.  \n",
    "Note the different dataloader in `i3d_resnet50-crop224_8x8_largejit_plateau_1scrop5tcrop_split1.py` and `trn_resnet50-crop224_8frame_largejit_plateau_5scrop_split1.py` in `exp_configs/hmdb`.  \n",
    "\n",
    "\n",
    "### LR policy\n",
    "The new exp_config has an optimiser policy. Refer to `get_optim_policies(model)` in the config file.  \n",
    "This lets you set different learning rate for different layers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66708ef-fcda-4a81-abb8-3d9acc425853",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%run \"$PYVIDEOAI_DIR/tools/run_train.py\" --local_world_size 1 -D hmdb -M trn_resnet50 -E crop224_8frame_largejit_plateau_5scrop_split1 -e 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e226b26-0958-4efd-9c39-1beee47e480a",
   "metadata": {},
   "source": [
    "## Changing dataset or model\n",
    "To change dataset, simply change the folder.  \n",
    "This changes the dataset_config to `dataset_configs/something_v1.py`, but keep other settings unchanged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3cdce0c-7cbe-42f3-af51-665d274b978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can replace `cp` to `ln -s` if you want to link the two files.\n",
    "!cp \"$PYVIDEOAI_DIR/exp_configs/hmdb/trn_resnet50-crop224_8frame_largejit_plateau_5scrop_split1.py\" \"$PYVIDEOAI_DIR/exp_configs/something_v1/trn_resnet50-crop224_8frame_largejit_plateau_5scrop_split1.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1936aaa-ab3f-4082-acb8-574e94e4a75b",
   "metadata": {},
   "source": [
    "To run TSM, you can simply copy the exp_config.  \n",
    "This changes the model_config to `model_configs/tsm_resnet50.py`, but keep other settings such as dataloader and optimiser unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "665a2f18-51c9-4f06-8fbd-9f2afdd640df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can replace `cp` to `ln -s` if you want to link the two files.\n",
    "!cp \"$PYVIDEOAI_DIR/exp_configs/hmdb/trn_resnet50-crop224_8frame_largejit_plateau_5scrop_split1.py\" \"$PYVIDEOAI_DIR/exp_configs/hmdb/tsm_resnet50-crop224_8frame_largejit_plateau_5scrop_split1.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd9d9a-cebf-4603-8b14-0bf4b9d37343",
   "metadata": {},
   "source": [
    "### Importing from another config file\n",
    "\n",
    "Okay, cool. But is copying the only way to change experiment settings? What if you have 1000 experiments and later they're too difficult to explore what the changed settings are?\n",
    "\n",
    "Indeed. Copying too many codes is the easiest way to make mistakes.  \n",
    "The other way is to \"import\" the other config file and change only the part of the configuration.  \n",
    "However, normal Python importing wouldn't work.\n",
    "\n",
    "```python\n",
    "# Example exp_config that loads config from another one.\n",
    "# Good idea, but wouldn't work.\n",
    "from trn_resnet50-crop224_8frame_largejit_plateau_5scrop_split1 import *\n",
    "input_frame_length = 32\n",
    "```\n",
    "\n",
    "The workaround is to use `exec()`.  \n",
    "Open a new config file like `exp_configs/hmdb/trn_resnet50-32frame.py` in an editor and try below.  \n",
    "```python\n",
    "# Example exp_config that loads config from another one. Keep all settings except the number of frames.\n",
    "# Right way\n",
    "import os\n",
    "_SCRIPT_DIR = os.path.dirname(os.path.abspath( __file__ ))\n",
    "exec(open(f'{_SCRIPT_DIR}/trn_resnet50-crop224_8frame_largejit_plateau_5scrop_split1.py').read())\n",
    "\n",
    "input_frame_length = 32\n",
    "```\n",
    "\n",
    "### Change config without copying. Use one config for all!\n",
    "\n",
    "What if your task is to find the best hyperparameter and you want to try 1000 different learning rates?  \n",
    "Do you create 1000 config files and write learning rate manually?  \n",
    "Here's a better way to do so: use ENVIRONMENT VARIABLES!\n",
    "\n",
    "Notice that the learning rate in the exp_config is defined as\n",
    "```python\n",
    "base_learning_rate = float(os.getenv('BASE_LR', 1e-5))\n",
    "```\n",
    "in `optimiser()`. Thus, you can overwrite the learning rate by setting the `BASE_LR` environment variable, or the default will be 1e-5.\n",
    "\n",
    "Your bash script may look like:\n",
    "```bash\n",
    "for LR in {1..10}    # 1e-1, 1e-2, ..., 1e-10\n",
    "do\n",
    "    BASE_LR=1e-$LR CUDA_VISIBLE_DEVICES=0 ~/PyVideoAI/tools/run_train.py --local_world_size 1 \\\n",
    "    -D hmdb -M trn_resnet50 -E crop224_8frame_largejit_plateau_5scrop_split1 \\\n",
    "    -R ~/experiments/base_lr-1e-$LR\n",
    "done\n",
    "```\n",
    "\n",
    "`-R` argument changes the experiment output folder location.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
