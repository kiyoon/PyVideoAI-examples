{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44eb80d-bdd5-4f1f-954f-c8c10301d679",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now the fun part begins. **Let's train the I3D network on the HMDB-51 dataset!**  \n",
    "\n",
    "First, we want to download the pre-trained weights from the [MODEL_ZOO.md](https://github.com/kiyoon/PyVideoAI/blob/master/MODEL_ZOO.md)  \n",
    "We'll use the I3D pretrained on the Kinetics-400 dataset, with 8-frame input.  \n",
    "\n",
    "Note that the path to the pretrained weights is defined in `model_configs/i3d_resnet50.py` as below.  \n",
    "```python\n",
    "kinetics400_pretrained_path_8x8 = os.path.join(DATA_DIR, 'pretrained', 'kinetics400/i3d_resnet50/I3D_8x8_R50.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b00c0c1-2cb7-41fa-aff9-916f779a8cda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYVIDEOAI_DIR=/home/kiyoon/project/PyVideoAI\n",
      "env: DATA_DIR=/home/kiyoon/project/PyVideoAI/data\n",
      "env: HDD_PATH=/storage/kiyoon\n"
     ]
    }
   ],
   "source": [
    "# Environments for future use\n",
    "\n",
    "from pyvideoai.config import PYVIDEOAI_DIR, DATA_DIR\n",
    "%env PYVIDEOAI_DIR=$PYVIDEOAI_DIR\n",
    "%env DATA_DIR=$DATA_DIR\n",
    "\n",
    "# !! CHANGE BELOW\n",
    "%env HDD_PATH=/storage/kiyoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6efdac-059c-46d4-a419-8856d6b8c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-14 03:17:54--  https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/I3D_8x8_R50.pkl\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 224598662 (214M) [application/octet-stream]\n",
      "Saving to: ‘/home/kiyoon/project/PyVideoAI/data/pretrained/I3D_8x8_R50.pkl’\n",
      "\n",
      "I3D_8x8_R50.pkl     100%[===================>] 214.19M  11.4MB/s    in 20s     \n",
      "\n",
      "2021-06-14 03:18:15 (10.6 MB/s) - ‘/home/kiyoon/project/PyVideoAI/data/pretrained/I3D_8x8_R50.pkl’ saved [224598662/224598662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Link the pretrained weight directory to HDD\n",
    "!mkdir -p \"$HDD_PATH/pretrained/kinetics400/i3d_resnet50\"\n",
    "!ln -s \"$HDD_PATH/pretrained\" \"$DATA_DIR/\"\n",
    "\n",
    "# Download\n",
    "!wget https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/I3D_8x8_R50.pkl -P \"$DATA_DIR/pretrained/kinetics400/i3d_resnet50\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c762f-6e2b-4841-91c3-82a299e00600",
   "metadata": {},
   "source": [
    "Now we have our datasets processed, and the pretrained weights ready.  \n",
    "We want to use three config files.  \n",
    "- `hmdb.py` in `dataset_configs`,  \n",
    "- `i3d_resnet50.py` in `model_configs`,  \n",
    "- `hmdb/i3d_resnet50-crop224_lr00_8x8_largejit_plateau_1scrop5tcrop_split1.py` in `exp_configs`.\n",
    "\n",
    "NOTE: The only difference between the inference example is the `load_pretrained(model)` function definition in the exp_config file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790f1cd2-a928-45fa-8bc6-3b9058d2f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Setup Telegram bot to report the experiment stats\n",
    "import os\n",
    "if not os.path.isfile(f\"{PYVIDEOAI_DIR}/tools/key.ini\"):\n",
    "    !cp \"$PYVIDEOAI_DIR/tools/key.ini\"{.template,}\n",
    "# EDIT the `tools/key.ini` file on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bf4ff2-68f2-4f49-a76a-eef25f45ef8a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment_utils.experiment_builder:  103 - INFO - Telegram bot initialised with keys in /home/kiyoon/project/PyVideoAI/tools/key.ini and using the bot number 0\n",
      "pyvideoai.train_multiprocess:  121 - INFO - git hash: 48a5d20dc2e17fcd7b4343cc562f043b9839d6e2\n",
      "pyvideoai.train_multiprocess:  125 - INFO - args: {\n",
      "    \"local_world_size\": 1,\n",
      "    \"shard_id\": 0,\n",
      "    \"num_shards\": 1,\n",
      "    \"init_method\": \"tcp://localhost:19999\",\n",
      "    \"backend\": \"nccl\",\n",
      "    \"num_epochs\": 100,\n",
      "    \"experiment_root\": \"/home/kiyoon/project/PyVideoAI/data/experiments\",\n",
      "    \"dataset\": \"hmdb\",\n",
      "    \"model\": \"i3d_resnet50\",\n",
      "    \"experiment_name\": \"crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1\",\n",
      "    \"dataset_channel\": null,\n",
      "    \"model_channel\": null,\n",
      "    \"experiment_channel\": null,\n",
      "    \"save_mode\": \"last_and_peaks\",\n",
      "    \"load_epoch\": null,\n",
      "    \"seed\": 12,\n",
      "    \"multi_crop_val_freq\": 5,\n",
      "    \"telegram_post_freq\": 5,\n",
      "    \"telegram_bot_idx\": 0,\n",
      "    \"dataloader_num_workers\": 4\n",
      "}\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset train...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 3570) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/train1.csv\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset test...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 1530) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/test1.csv\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset test...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 7650) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/test1.csv\n",
      "pyvideoai.utils.misc:   52 - INFO - Model:\n",
      "ResNetModel(\n",
      "  (s1): VideoModelStem(\n",
      "    (pathway0_stem): ResNetBasicStem(\n",
      "      (conv): Conv3d(3, 64, kernel_size=[5, 7, 7], stride=[1, 2, 2], padding=[2, 3, 3], bias=False)\n",
      "      (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (pool_layer): MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "-----------------------------------------------------------\n",
      "----------OUTPUT SIMPLIFIED FOR NOTEBOOK VIEWERS-----------\n",
      "-----------------------------------------------------------\n",
      "  (head): ResNetBasicHead(\n",
      "    (pathway0_avgpool): AvgPool3d(kernel_size=[4, 7, 7], stride=1, padding=0)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (projection): Linear(in_features=2048, out_features=51, bias=True)\n",
      "    (act): Softmax(dim=4)\n",
      "  )\n",
      ")\n",
      "pyvideoai.utils.misc:   53 - INFO - Params: 27,328,371\n",
      "pyvideoai.utils.misc:   54 - INFO - Mem: 104.4853515625 MiB\n",
      "pyvideoai.utils.misc:   55 - INFO - nvidia-smi\n",
      "pyvideoai.utils.misc:   56 - INFO - Tue Jun 15 07:03:50 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3090    Off  | 00000000:23:00.0 Off |                  N/A |\n",
      "| 55%   52C    P2   120W / 370W |   1213MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 3090    Off  | 00000000:2D:00.0 Off |                  N/A |\n",
      "| 53%   60C    P0    45W / 370W |      0MiB / 24251MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1353154      C   ...3/envs/videoai/bin/python     1211MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res_conv1_bn_b: (64,) => s1.pathway0_stem.bn.bias: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res_conv1_bn_s: (64,) => s1.pathway0_stem.bn.weight: (64,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res5_2_branch2a_bn_rm: (512,) => s5.pathway0_res2.branch2.a_bn.running_mean: (512,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_4_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res4.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "-----------------------------------------------------------\n",
      "----------OUTPUT SIMPLIFIED FOR NOTEBOOK VIEWERS-----------\n",
      "-----------------------------------------------------------\n",
      "pyvideoai.slowfast.utils.checkpoint:  204 - WARNING - !! pred_w: (400, 2048) does not match head.projection.weight: (51, 2048)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2a_w: (256, 1024, 1, 1, 1) => s4.pathway0_res3.branch2.a.weight: (256, 1024, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_1_branch2b_w: (256, 256, 1, 3, 3) => s4.pathway0_res1.branch2.b.weight: (256, 256, 1, 3, 3)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_3_branch2c_w: (1024, 256, 1, 1, 1) => s4.pathway0_res3.branch2.c.weight: (1024, 256, 1, 1, 1)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2a_bn_s: (256,) => s4.pathway0_res5.branch2.a_bn.weight: (256,)\n",
      "pyvideoai.slowfast.utils.checkpoint:  195 - INFO - res4_5_branch2a_bn_b: (256,) => s4.pathway0_res5.branch2.a_bn.bias: (256,)\n",
      "pyvideoai.train_multiprocess:  220 - INFO - cfg.criterion not defined. Using CrossEntropyLoss()\n",
      "pyvideoai.train_multiprocess:  248 - INFO - ReduceLROnPlateau scheduler is selected. The `scheduler.step(val_loss)` function will be called at the end of epoch (after validation), but not every iteration.\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 0/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 3.8874 - loss: 3.8073 - batch_acc: 0.1250 - acc: 0.1085        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 3.8073 - acc: 0.1085                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.9147 - val_acc: 0.3732        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.9147 - val_acc: 0.3732\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0000.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 1/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 3.5042 - loss: 3.4460 - batch_acc: 0.2500 - acc: 0.3310        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 3.4460 - acc: 0.3310                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.8353 - val_acc: 0.4196        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.8353 - val_acc: 0.4196\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0001.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0000.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 2/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00010000 - batch_loss: 2.3087 - loss: 3.0303 - batch_acc: 0.8750 - acc: 0.4268        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00010000 - loss: 3.0303 - acc: 0.4268                                                            \n",
      "-----------------------------------------------------------\n",
      "----------OUTPUT SIMPLIFIED FOR NOTEBOOK VIEWERS-----------\n",
      "-----------------------------------------------------------\n",
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3184 - val_acc: 0.7033\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0097.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0096.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 98/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 1.1730 - loss: 0.5969 - batch_acc: 0.6250 - acc: 0.8461        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5969 - acc: 0.8461                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3158 - val_acc: 0.7046        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3158 - val_acc: 0.7046\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0098.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0097.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 99/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.8652 - loss: 0.5864 - batch_acc: 0.7500 - acc: 0.8422        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 96s - lr: 0.00000001 - loss: 0.5864 - acc: 0.8422                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3208 - val_acc: 0.7033        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3208 - val_acc: 0.7033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - ETA:    0s - val_loss: 3.3233 - val_acc: 0.7082        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3233 - val_acc: 0.7082 - val_vid_acc_top1: 0.7301 - val_vid_acc_top5: 0.9405\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0099.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0098.pth\n",
      "pyvideoai.train_multiprocess:  455 - SUCCESS - Finished training\n"
     ]
    }
   ],
   "source": [
    "# If you have more than 1 GPUs, go ahead and change CUDA_VISIBLE_DEVICES to the comma-separated GPU indices, and --local_world_size to the number of GPUs.\n",
    "# This will increase the batch size (1 GPU=size 8, 2 GPU=size 16, ...) and speed up the learning.\n",
    "# However, Jupyter Notebook doesn't seem to support stdout printing with multiple processes. Try using normal shell.\n",
    "\n",
    "# -e sets the number of epochs.\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%run \"$PYVIDEOAI_DIR/tools/run_train.py\" --local_world_size 1 -D hmdb -M i3d_resnet50 -E crop224_lr0001_8x8_largejit_plateau_1scrop5tcrop_split1 -e 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ff61a-551c-40b7-9928-a80f91fdb7fc",
   "metadata": {},
   "source": [
    "### Multicrop evaluation\n",
    "Since one clip is too short to see the entire video, it will perform multicrop (1 centre crop, 5 temporal crops) evaluation every 5 epochs (`-t 5`).  \n",
    "FYI, most Facebook AI Research papers (e.g. SlowFast) use 3 spatial crops and 10 temporal crops (totalling 30 clips per video).  \n",
    "To change the frequency, add `-t EPOCHS`.  \n",
    "To skip the multicrop evaluation, add `-t -1`.  \n",
    "To change the number of spatial and temporal crops, edit `test_num_spatial_crops` and `test_num_ensemble_views` in `exp_configs/hmdb/i3d_resnet50-crop224_lr0001_8x8_largejit_plateau_1scrop5tcrop_split1.py`.  \n",
    "Or, you can change the dataloader by changing `def get_torch_dataset(split)` function in the config file.\n",
    "\n",
    "### Saving model weights\n",
    "It will only keep the models with peak validation accuracy, plus the last epoch. That's why you see that it's removing the previous model when it's not better than current.  \n",
    "This is due to the command line argument `--save_mode last_and_peaks` which is set by default.  \n",
    "Use `--save_mode all` in order to keep checkpoints from every epoch.  \n",
    "\n",
    "### Experiment output structure\n",
    "The output directory will be organised as follows:  \n",
    "\n",
    "```\n",
    "${experiment_root}\n",
    "└── ${dataset}\n",
    "    └── ${model_name}\n",
    "        └── ${experiment_name}\n",
    "            ├── configs\n",
    "            │   └── args.json\n",
    "            ├── logs\n",
    "            │   └── summary.csv\n",
    "            │   └── train.log\n",
    "            ├── plots\n",
    "            │   ├── accuracy.pdf\n",
    "            │   ├── loss.pdf\n",
    "            │   ├── video_accuracy_top1.pdf\n",
    "            │   └── video_accuracy_top5.pdf\n",
    "            ├── (predictions)\n",
    "            ├── tensorboard_runs\n",
    "            └── weights\n",
    "                ├── epoch_0000.pth\n",
    "                └── epoch_0001.pth\n",
    "```\n",
    "\n",
    "\n",
    "### Visualisation\n",
    "#### Telegram\n",
    "If you've set up the Telegram bot, it will report the training stats (and even get notifications of errors running the script) like this:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/12980409/122335586-7cb10a80-cf76-11eb-950f-af08c20055d4.png\" alt=\"Telegram bot stat report example\" width=\"400\">\n",
    "\n",
    "#### TensorBoard\n",
    "You can use TensorBoard in `data/experiments/hmdb/i3d_resnet/crop224_lr0001_8x8_largejit_steplr_1scrop5tcrop_split1/tensorboard_runs`.  \n",
    "[Example tensorboard link](https://tensorboard.dev/experiment/mGSBcdZfQmWJNd658zHLbQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec71f3-d524-4ca6-af49-4af6d4c9eb72",
   "metadata": {},
   "source": [
    "## Evaluate using the saved weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb7f31-dbb6-486c-97a8-99de40fa1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -p will save the predictions into a pickle file.\n",
    "# -l -2 will pick the best model (highest validation accuracy).\n",
    "# -l -1 will pick the last model\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%run \"$PYVIDEOAI_DIR/tools/run_val.py\" --local_world_size 1 -D hmdb -M i3d_resnet50 -N crop224_lr0001_8x8_largejit_plateau_1scrop5tcrop_split1 -l -2 -p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb811fc-9c96-4ec5-ae1f-31b2d021451b",
   "metadata": {},
   "source": [
    "## Resume training\n",
    "\n",
    "### From the last checkpoint\n",
    "By simply adding `-l -1`, it will resume from the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "274ae7ad-65e1-439d-a998-7c7d368e7a1b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment_utils.experiment_builder:  103 - INFO - Telegram bot initialised with keys in /home/kiyoon/project/PyVideoAI/tools/key.ini and using the bot number 0\n",
      "pyvideoai.train_multiprocess:  121 - INFO - git hash: 48a5d20dc2e17fcd7b4343cc562f043b9839d6e2\n",
      "pyvideoai.train_multiprocess:  125 - INFO - args: {\n",
      "    \"local_world_size\": 1,\n",
      "    \"shard_id\": 0,\n",
      "    \"num_shards\": 1,\n",
      "    \"init_method\": \"tcp://localhost:19999\",\n",
      "    \"backend\": \"nccl\",\n",
      "    \"num_epochs\": 200,\n",
      "    \"experiment_root\": \"/home/kiyoon/project/PyVideoAI/data/experiments\",\n",
      "    \"dataset\": \"hmdb\",\n",
      "    \"model\": \"i3d_resnet50\",\n",
      "    \"experiment_name\": \"crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1\",\n",
      "    \"dataset_channel\": null,\n",
      "    \"model_channel\": null,\n",
      "    \"experiment_channel\": null,\n",
      "    \"save_mode\": \"last_and_peaks\",\n",
      "    \"load_epoch\": -1,\n",
      "    \"seed\": 12,\n",
      "    \"multi_crop_val_freq\": 5,\n",
      "    \"telegram_post_freq\": 5,\n",
      "    \"telegram_bot_idx\": 0,\n",
      "    \"dataloader_num_workers\": 4\n",
      "}\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset train...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 3570) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/train1.csv\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset test...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 1530) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/test1.csv\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:   95 - INFO - Constructing video dataset test...\n",
      "pyvideoai.dataloaders.frames_densesample_dataset:  140 - INFO - Constructing video dataloader (size: 7650) from /home/kiyoon/project/PyVideoAI/data/hmdb51/splits_frames/test1.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.utils.misc:   52 - INFO - Model:\n",
      "ResNetModel(\n",
      "  (s1): VideoModelStem(\n",
      "    (pathway0_stem): ResNetBasicStem(\n",
      "      (conv): Conv3d(3, 64, kernel_size=[5, 7, 7], stride=[1, 2, 2], padding=[2, 3, 3], bias=False)\n",
      "      (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (pool_layer): MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "-----------------------------------------------------------\n",
      "----------OUTPUT SIMPLIFIED FOR NOTEBOOK VIEWERS-----------\n",
      "-----------------------------------------------------------\n",
      "  (head): ResNetBasicHead(\n",
      "    (pathway0_avgpool): AvgPool3d(kernel_size=[4, 7, 7], stride=1, padding=0)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (projection): Linear(in_features=2048, out_features=51, bias=True)\n",
      "    (act): Softmax(dim=4)\n",
      "  )\n",
      ")\n",
      "pyvideoai.utils.misc:   53 - INFO - Params: 27,328,371\n",
      "pyvideoai.utils.misc:   54 - INFO - Mem: 4,840.8818359375 MiB\n",
      "pyvideoai.utils.misc:   55 - INFO - nvidia-smi\n",
      "pyvideoai.utils.misc:   56 - INFO - Wed Jun 16 00:02:21 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3090    Off  | 00000000:23:00.0 Off |                  N/A |\n",
      "|  0%   34C    P2    86W / 370W |   6365MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 3090    Off  | 00000000:2D:00.0 Off |                  N/A |\n",
      "| 38%   46C    P0    40W / 370W |      0MiB / 24251MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1353154      C   ...3/envs/videoai/bin/python     6363MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "pyvideoai.utils.loader:   93 - INFO - Loading weights: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0099.pth\n",
      "pyvideoai.train_multiprocess:  220 - INFO - cfg.criterion not defined. Using CrossEntropyLoss()\n",
      "pyvideoai.train_multiprocess:  236 - INFO - cfg.load_optimiser_state not found. By default, it will try to load.\n",
      "pyvideoai.train_multiprocess:  240 - INFO - Loading optimiser state from the checkpoint.\n",
      "pyvideoai.train_multiprocess:  248 - INFO - ReduceLROnPlateau scheduler is selected. The `scheduler.step(val_loss)` function will be called at the end of epoch (after validation), but not every iteration.\n",
      "pyvideoai.train_multiprocess:  250 - INFO - For the ReduceLROnPlateau scheduler, initialise using training stats (summary.csv) instead of the checkpoint.\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 100/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Iter:  446/ 446 - Sample:   3568/  3570 - ETA:    0s - lr: 0.00000001 - batch_loss: 0.9854 - loss: 0.5750 - batch_acc: 0.6250 - acc: 0.8487        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  146 - INFO -  Train Iter:  446/ 446 - Sample:   3568/  3570 - 95s - lr: 0.00000001 - loss: 0.5750 - acc: 0.8487                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - ETA:    0s - val_loss: 3.3217 - val_acc: 0.7085        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  One-clip Eval Iter:  192/ 192 - Sample:   1530/  1530 - 15s - val_loss: 3.3217 - val_acc: 0.7085\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0100.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0099.pth\n",
      "pyvideoai.train_multiprocess:  333 - INFO - Epoch 101/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "----------OUTPUT SIMPLIFIED FOR NOTEBOOK VIEWERS-----------\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyvideoai.train_and_val:  324 - INFO -  Multi-clip Eval Iter:  957/ 957 - Sample:   7650/  7650 - 75s - val_loss: 3.3233 - val_acc: 0.7082 - val_vid_acc_top1: 0.7294 - val_vid_acc_top5: 0.9405\n",
      "pyvideoai.train_multiprocess:  424 - INFO - Saving model to /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0199.pth\n",
      "pyvideoai.train_multiprocess:  445 - INFO - Removing previous model: /home/kiyoon/project/PyVideoAI/data/experiments/hmdb/i3d_resnet50/crop224_lr0001_batch8_8x8_largejit_plateau_1scrop5tcrop_split1/weights/epoch_0198.pth\n",
      "pyvideoai.train_multiprocess:  455 - SUCCESS - Finished training\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%run \"$PYVIDEOAI_DIR/tools/run_train.py\" --local_world_size 1 -D hmdb -M i3d_resnet50 -N crop224_lr0001_8x8_largejit_plateau_1scrop5tcrop_split1 -e 200 -l -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c952349-711a-43bd-bc4c-b209e876fd4b",
   "metadata": {},
   "source": [
    "### From the intermediate checkpoint\n",
    "Go to the experiment folder, and find the summary file in `data/experiments/hmdb/i3d_resnet/crop224_lr0001_8x8_largejit_steplr_1scrop5tcrop_split1/logs/summary.csv`.  \n",
    "You'll see the training stats. Just remove the lines after your resuming point.  \n",
    "For example, you want to resume from the 50th epoch and start from 51st epoch, then remove from epoch 51 till the end.  \n",
    "Add `-l 50` to load the 50th epoch's checkpoint and resume from the 51st epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f746e-cb29-4c2a-8266-8296ab0de534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: Before running the code, make sure you change the summary.csv file!!\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%run \"$PYVIDEOAI_DIR/tools/run_train.py\" --local_world_size 1 -D hmdb -M i3d_resnet50 -N crop224_lr0001_8x8_largejit_steplr_1scrop5tcrop_split1 -e 200 -l 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536fb9c-bcdb-47a2-824a-ce97f9dc0bac",
   "metadata": {},
   "source": [
    "# Running TSN / TRN\n",
    "\n",
    "Let's train using different model with the same dataset.  \n",
    "\n",
    "\n",
    "### Dense sampling vs Sparse sampling\n",
    "The difference between the I3D and the TRN is that the former is sampling videos densely, and the latter is sparsely.  \n",
    "Note the different dataloader in `i3d_resnet50-crop224_lr0001_8x8_largejit_plateau_1scrop5tcrop_split1.py` and `trn_resnet50-crop224_lr0001_8frame_largejit_plateau_5scrop_split1.py` in `exp_configs/hmdb`.  \n",
    "In sparse sampling mechanism, multicrop operates only spatially, and to me it didn't make much difference compared to temporal multicrop. Thus, we disable it.  \n",
    "\n",
    "\n",
    "### Early stopping & LR policy\n",
    "The new exp_config has early stopping and LR policy. Refer to the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66708ef-fcda-4a81-abb8-3d9acc425853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -t -1 disables multicrop evaluation\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%run \"$PYVIDEOAI_DIR/tools/run_train.py\" --local_world_size 1 -D hmdb -M trn_resnet50 -N crop224_lr0001_8frame_largejit_plateau_5scrop_split1 -e 100 -t -1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
